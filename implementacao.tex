\chapter{Implementação}
\label{chap:impl}

Neste capítulo discutiremos tópicos relacionados à implementação do sistema de diarização de locutor proposto. 

Na seção \ref{sec:tools} apresentamos as ferramentas principais utilizadas no desenvolvimento deste trabalho.
Em seguida, na seção \ref{sec:preproc}, apresentamos o trabalho realizado para preparação e pré-processamento dos dados.

Foram desenvolvidos dois conjuntos scripts distintos para este trabalho; um sistema para treinamento da rede neural convolucional, e outro para a diarização de locutor em um vídeo com as características definidas anteriormente.
Sendo assim, discutiremos a arquitetura utilizada para treinamento do modelo na seção \ref{sec:train}, e a utilizada no processamento de uma mídia de entrada na seção \ref{sec:application}.

\section{Ferramentas}
\label{sec:tools}

Nesta seção apresentamos as principais ferramentas utilizadas durante a implementação deste trabalho.
Definiremos suas principais características, assim como as funcionalidades das mesmas que foram utilizadas, e as motivações por trás de sua escolha.

Na seção \ref{subsec:dlib} apresentaremos a biblioteca Dlib, utilizada para propósito de identificação e demarcação facial no projeto.
Em seguida, na seção \ref{subsec:tf} apresentaremos a biblioteca Tensorflow, utilizada por sua robusta implementação do modelo de rede neural convolucional.
Na seção \ref{subsec:environ} apresentaremos o ambiente utilizado para treinamento do modelo, assim como seus recursos computacionais.
Por fim, na seção \ref{subsec:otools}, mencionaremos brevemente as demais ferramentas utilizadas em caráter pontual no trabalho, e que, por essa razão, não receberam seções dedicadas.

\subsection{Dlib}
\label{subsec:dlib}
A Dlib\cite{dlib09} é uma biblioteca de código aberto desenvolvida em C++ e com interface em Python.
Trata-se de uma biblioteca generalista, contendo implementações de algoritmos para processamento paralelo, grafos, entre outros.
Porém, seu foco principal se encontra nas áreas de aprendizado de máquina, processamento de imagem e reconhecimento facial.

A biblioteca possui um identificador facial baseado em \textit{Histogram of Oriented Gradients} (HOG) capaz de detectar rostos frontais mesmo em imagens de baixa resolução.
As características específicas deste reconhecedor foram discutidas na seção \ref{sec:faciallm}.

% FIXME: Colocar mais aprofundadamente na fundamentação teórica
% HOG é uma técnica de visão computacional na qual são calculados gradientes para cada pixel da imagem a partir de seus vizinhos de forma a identificar as fronteiras entre os objetos da figura. Os gradientes calculados são então utilizados para computar blocos descritores de 8x8 pixels com 9 canais. Esses blocos sofrem normalização local para compensar fatores como a iluminação do ambiente, e servem como entrada para uma Máquina de Vetores de Suporte, resultando em um reconhecedor de faces rápido e robusto. Uma explicação mais aprofundada sobre esta técnica pode ser obtida em \cite{dalalHistogramsOrientedGradients2005}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{dlib-face_landmark_detection.jpg}    
    \caption{Detecção de Marcadores Faciais pela biblioteca Dlib. Imagem publicada sob licença Creative Commons\cite{mtheilerDeteccaoMarcadoresFaciais2019}. }
    \label{fig:dlib-landmarking}
\end{figure}

Além disso, a mesma implementa um detector de marcadores faciais baseado em uma floresta de árvores de regressão, capaz de identificar (ou estimar, caso não estejam visíveis) as posições de um conjunto de pontos em um rosto, como mostra a figura \ref{fig:dlib-landmarking}.
A biblioteca fornece também um modelo pré-treinado para este detector para identificação de 68 destes marcadores, sob licença que permite uso acadêmico.
Esta funcionalidade foi chave para a escolha da biblioteca para a implementação do trabalho, visto que permitiu o treinamento do classificador utilizando um conjunto menor de dados, sem preocupações quanto a vieses relacionados às características físicas do locutor.

A biblioteca Dlib foi utilizada em sua versão 19.19.0, compilada a partir do código fonte com suporte para GPU e otimizações referentes à arquitetura da CPU.

\subsection{Tensorflow}
\label{subsec:tf}

Tensorflow\cite{tensorflow2015-whitepaper} é um framework de código aberto para aplicações de aprendizado de máquina.
A biblioteca, construída pela empresa Google, apresenta implementações robustas de diversos algoritmos da área, além de uma API que permite a declaração de uma rede neural em função de suas camadas.
A biblioteca suporta, ainda, o uso de uma ou mais GPUs para treinamento da rede neural, através da biblioteca cuDNN. No trabalho, essa foi utilizada para assistir na modelagem da rede neural, assim como seu treinamento e posterior execução como parte do sistema completo.

A escolha desta biblioteca se deu devido à sua implementação de algoritmos chave para o desenvolvimento do trabalho, tais como a rede neural convolucional tridimensional, discutida na seção \ref{sec:ann}.
Além disso, suas interfaces nas linguagens de programação C++ e Python, assim como a facilidade de utilização de interface em Python para prototipagem rápida de redes neurais convolucionais com topologias diferentes foram decisivas para sua escolha para a realização do trabalho.

Nesse trabalho, utilizamos a versão do Tensorflow 2.1.0, compilada a partir do código fonte com suporte para GPU e otimizações do referentes à arquitetura da CPU.

\subsection{Ambiente de Desenvolvimento}
\label{subsec:environ}

No desenvolvimento deste trabalho foi utilizada em caráter primário a linguagem de programação Python.
Originalmente, o trabalho seria desenvolvido em C++ devido ao mais alto desempenho desta linguagem; no entanto, a utilização de diversas bibliotecas com interface em Python assim como o mais rápido desenvolvimento e prototipagem nesta linguagem levou à decisão final de utiliza-la para a implementação do projeto.

A IDE utilizada no desenvolvimento foi o Visual Studio Code, ferramenta de código aberto criada pela Microsoft, e o Jupyter Notebook\cite{kluyverJupyterNotebooksPublishing2016}, por sua capacidade de subdividir e visualizar o estado do programa em execução.
%O treinamento da rede neural foi realizado em máquina com sistema operacional Windows 10, equipada com uma CPU Intel Core i7 de quarta geração, 12 GB de memória RAM e uma GPU Nvidia GTX 980 com 4 GB de memória dedicada.
O treinamento da rede neural foi realizado em máquina com sistema operacional Ubuntu Linux 16.04, equipada com uma CPU Intel Core i7 de primeira geração, 12 GB de memória RAM, e uma GPU Nvidia GTX 1080 Ti com 12 GB de memória dedicada.

\subsection{Outras Ferramentas}
\label{subsec:otools}

Nesta seção apresentamos as demais bibliotecas utilizadas no desenvolvimento do projeto. Em cada subseção descrevemos brevemente a biblioteca, definindo seu papel no projeto.

As bibliotecas se encontram ordenadas por sua função na pipeline do classificador, discutida de forma mais aprofundada na seção \ref{sec:train}.

\subsubsection{OpenCV}

OpenCV\cite{opencv_library} é uma biblioteca de código aberto para aplicações de Visão Computacional.
Ela foi utilizada para realizar a leitura quadro a quadro dos arquivos de vídeo a serem processados pelo sistema, e para codificar em vídeo a saída do classificador.
Neste trabalho foi utilizada a biblioteca opencv-python em sua versão 4.2.0.32.

\subsubsection{Matplotlib}

A Matplotlib\cite{hunterMatplotlib2DGraphics2007} é uma biblioteca para produção de gráficos e imagens em Python.
Ela foi utilizada para produzir as imagens intermediárias, através do desenho de polígonos a partir dos vértices produzidos pelo processo de detecção de marcadores facial.
Neste trabalho utilizamos a Matplotlib na versão 3.1.3 da biblioteca.

\subsubsection{Pandas}

Pandas\cite{mckinney-proc-scipy-2010} é uma biblioteca de processamento de dados em Python.
Ela foi utilizada no pré-processamento dos dados com a finalidade de manipular os arquivos csv contendo a diarização manual dos vídeos do dataset de depoimentos.
A biblioteca foi utilizada em sua versão 1.0.0.

\subsubsection{Scikit Learn e dscore}

As bibliotecas Scikit-Learn\cite{pedregosaScikitlearnMachineLearning2011}, biblioteca de aprendizado de máquina tradicional em python, e dscore\cite{ryantNryantDscore2020}, biblioteca para cálculo de métricas de diarização, foram utilizadas para o cálculo de diversas métricas relacionadas ao desempenho do sistema.
Essas foram utilizadas em suas versões 0.22.2.post1 e 1.1.0, respectivamente.

\section{Preparação dos Dados}
\label{sec:preproc}

Originalmente, foi fornecido pela Defensoria Pública do Estado do Rio de Janeiro um dataset contendo 29 vídeos, totalizando cerca de 5 horas de vídeo, referente a depoimentos prestados por diferentes participantes.
Os vídeos fornecidos apresentam resolução de $320\times240$ pixels, a uma taxa de 30 quadros por segundo.
O conjunto de dados não apresentava nenhuma anotação referente à fala dos depoentes.

Primeiramente, segmentamos os vídeos em fragmentos de 15 quadros, ou meio segundo.
A motivação para a segmentação do vídeo de entrada em trechos deste comprimento foi a estipulação de que o tempo necessário para a fala da primeira sílaba em uma frase, por uma pessoa normal, no português do Brasil, é de 252 milissegundos\cite{barbosaSyllabletimingBrazilianPortuguese2000}.
Sendo assim, ao dobrar este tempo, adquirimos a capacidade de detectar por completo o movimento do locutor quando referente a frases curtas, tais como interjeições.

Estes trechos foram validados para determinar se a face do depoente poderia ser reconhecida, com finalidade de descartar fragmentos nos quais este não olhava em direção à câmera, ou nas quais o detector HOG não poderia identifica-los.
Cada trecho foi então manualmente classificado quanto à ocorrência de fala pelo depoente, produzindo uma tabela que associava o identificador de cada arquivo à classe correspondente ao mesmo.

Feita essa diarização manual, os vídeos foram separados em conjuntos de teste e treinamento, tais que vídeos diferentes seriam utilizados para cada fase do treinamento da rede neural.
Fizemos essa separação pois, como os fragmentos possuem relação temporal tanto entre si quanto com outros vídeos do mesmo depoente, seria possível que o treinamento da rede neural utilizando segmentos semelhantes aos fragmentos de teste pudesse inflar artificialmente o desempenho da mesma.

%Por fim, as diarizações produzidas manualmente foram utilizadas para organizar os fragmentos em uma estrutura de diretórios, tal que primeiramente estivessem separados por conjunto de teste ou treinamento, depois por vídeo de origem, e por fim por classe.
%Essa estrutura foi construída de tal forma que as informações relevantes ao gerador de amostras da rede neural pudessem ser obtidas todas a partir do caminho para o mesmo no sistema de arquivos.

Para a execução de todas essas tarefas foram desenvolvidos scripts em Python e Bash, capazes de segmentar os vídeos do dataset e de validar, mover, e organizar os fragmentos extraídos destes.
O código referente a estes pode ser encontrado no apêndice \ref{apdx:src}.

\section{Treinamento}
\label{sec:train}

Para o treinamento do modelo foi desenvolvido um sistema composto por dois componentes principais.

\begin{figure}[ht]
    \centering
    \fontsize{10pt}{10pt}\selectfont
    \includesvg[width=0.95\textwidth]{figures/architecture.svg}
    %\includegraphics[width=0.85\textwidth]{figures/architecture.png}
    \caption{A sequência de carregamento dos dados e treinamento do modelo.}
    \label{fig:arch_train}
\end{figure}

Primeiramente, foi construído um gerados de dados com a finalidade de alimentar os dados da entrada ao modelo dinamicamente. As características e funcionalidades se encontram descritas na seção \ref{sec:data-gen}.
Depois, foi definida a topologia do modelo de rede neural, discutida na seção \ref{sec:topology}, e os critérios e parâmetros relevantes ao fluxo de treinamento, discutidos na seção \ref{sec:train-flow}.

\subsection{Gerador de Dados}
\label{sec:data-gen}
A função primária do gerador de dados é carregar dinamicamente os vídeos a serem alimentados ao modelo em cada etapa de seu treinamento.
Um módulo capaz de realizar tal carregamento dinâmico foi necessário devido ao grande volume de dados sendo processados, por virtude de se tratarem de dados de vídeo.
Para isto, ele é inicializado com uma lista dos arquivos a serem carregados, assim como uma série de parâmetros que definem aspectos de sua operação, tais como o tamanho da batelada ou o pré-processamento a ser aplicado aos quadros.

Cada vídeo fornecido ao gerador é, ainda, invertido horizontalmente.
Isso é feito para garantir que o modelo fosse treinado para funcionar independentemente do ângulo do rosto do sujeito, desde que reconhecível pelo HOG, visto que na grande maioria dos vídeos do dataset original o falante se encontrava olhando para o lado esquerdo da câmera ou diretamente para esta.

Para acelerar o processo de identificação facial e detecção de marcadores faciais, o gerador é capaz de processar múltiplos fragmentos em paralelo na CPU, além de manter um cache dos fragmentos processados em iterações anteriores do treinamento.
Esse paralelismo possibilitou a redução do tempo de execução da primeira iteração do treinamento e teste de cerca de 9 horas para aproximadamente uma hora e trinta minutos, uma redução de 83\% no tempo de execução com 6 threads, enquanto a implementação do cache reduziu o tempo necessário para etapas subsequentes do processo de para cerca de 3 minutos.

\subsection{Pré-Processamento}
\label{sec:pre-processing}

O gerador de dados possui 3 modos distintos de operação, demonstrados na figura \ref{fig:generator_out}, que correspondem aos filtros de pré-processamento que podem ser aplicados aos quadros carregados para compor a entrada do modelo de predição. Estes são:

\begin{enumerate}[label={(\arabic*)}]
    \item RGB: Nesse modo, o quadro é codificado em padrão RGB, ou seja, com cores representadas por suas componentes vermelha, verde, e azul, nessa ordem.
    \item Grayscale: Nesse modo, o quadro RGB é convertido para escala de cinza, segundo a função $Y = 0.299 R + 0.587 G + 0.114 B$.
    \item Landmarks: Nesse modo, é gerada uma imagem em preto e branco, rasterizada a partir dos pontos identificados pela detecção de marcadores faciais em cada quadro carregado. Este foi o modo utilizado na implementação final do trabalho.
\end{enumerate}

\begin{figure}[ht]
    \centering
    \begin{enumerate}[label={(\arabic*)}]
        \item \parbox{\linewidth}{\centering
            \includesvg[width=0.9\textwidth]{loader_rgb.svg}
        }
        \item \parbox{\linewidth}{\centering
            \includesvg[width=0.9\textwidth]{loader_gray.svg}
        }
        \item \parbox{\linewidth}{\centering
            \includesvg[width=0.9\textwidth]{loader_lm.svg}
        }
    \end{enumerate}
    \caption{A saída do gerador de dados em cada um de seus três modos de operação.}
    \label{fig:generator_out}
\end{figure}

\begin{algorithm}[H]
    \SetAlgoLined
    \caption{Pré-processamento de um fragmento de vídeo (landmarks).}
    \label{alg:pre-processing}
    \KwResult{Sequência de 15 quadros pré-processados}
    \While{Quadros Carregados < 15}{
        Carrega quadro do fragmento\;
        Detecta faces no quadro carregado usando o detector facial (HOG)\;
        Calcula os 68 marcadores faciais utilizando o algoritmo de alinhamento facial\;
        Rasteriza os marcadores obtidos sobre um canvas branco\;
        Recorta o canvas segundo a região identificada pelo HOG\;
        Alinha horizontalmente a face detectada e amplia o desenho\;
    }
\end{algorithm}

Em nossa implementação final, utilizamos o modo de \textit{landmarks}. Esse modo implementa o algoritmo \ref{alg:pre-processing}.

\subsection{Topologia do Modelo}
\label{sec:topology}

Para o modelo de predição, definimos uma topologia baseada na arquitetura VGG\footnote{VGG é um estilo de arquitetura de rede neural convolucional para reconhecimento de objetos em fotos, desenvolvida pelo Grupo de Geometria Visual (do inglês \textit{Visual Geometry Group}) da universidade de Oxford \cite{simonyanVeryDeepConvolutional2015}.}, definido na figura \ref{fig:topology_typeA}, que consiste em um número de pares de camadas de convolução e \textit{max pooling}, seguidas de camadas densamente conectadas.
A arquitetura foi adaptada para lidar com dados de caráter tri-dimensional.
Nesta seção iremos discutir as decisões tomadas neste processo de adaptação da arquitetura, e os fatores que levaram a estas decisões.

\begin{figure}[ht]
    \centering
    \resizebox{!}{0.7\textheight}{
        \input{figures/model_typeA.tikz}
        }
    \caption{Topologia do Modelo}
    \label{fig:topology_typeA}
\end{figure}

Primeiramente, na primeira camada convolucional da rede, que frequentemente tem a função de detectar bordas na imagem de entrada, adicionamos uma componente temporal ao \kernel\ com dimensão 4.
Essa dimensão par não é usual para uma rede neural convolucional, devido ao fato de que não atua sobre um elemento da imagem original alterando-o em relação a seus vizinhos, produzindo ao invés disso um novo elemento a partir da média dos valores internos às regiões de tal dimensão, ponderada pelos pesos do \kernel\ da matriz de convolução.
Em nosso modelo isso corresponde a construir um conjunto de filtros capazes de identificar variações temporais relevantes em cada sequência de 4 quadros da entrada.

Na segunda camada de convolução, adicionamos uma componente temporal com dimensão 3, igual às demais.
Isto consiste em avaliar o valor de cada ponto da matriz de entrada em função dos valores de seus vizinhos, o que possibilita a identificação das áreas da matriz relevantes ao problema.
A baixa dimensão utilizada em relação ao tamanho da entrada faz com que as características observadas sejam características locais, tais como bordas e cantos nas quais ocorram as variações desejadas já previamente identificadas.

Na última camada de convolução, mantivemos caráter bi-dimensional do \kernel\ através da dimensão temporal igual a 1, apesar de constituir uma convolução tri-dimensional.
Isso corresponde a operar individualmente sobre os quadros da entrada, agora compostos pela combinação temporal dos quadros originais, buscando características destes que sejam relevantes para a classificação, a ser realizada em seguida pelas camadas densas.

Finalmente, o número de neurônios das camadas totalmente conectadas foi reduzido a partir dos valores originais propostos na VGG16.
Essa redução foi realizada de forma iterativa, buscando manter a acurácia do modelo constante e reduzir o \textit{overfitting} através da eliminação de parâmetros redundantes.

\subsection{Treinamento do Modelo}
\label{sec:train-flow}

Em cada época, o sistema de treinamento do modelo chama o gerador de dados de treinamento, que lhe retorna um número fixo de amostras (batelada) compostas por segmentos de 15 quadros consecutivos.
O modelo então faz suas predições quanto a estas, e tem seus pesos ajustados através do algoritmo de retro-propagação de forma a minimizar uma função de custo.

\begin{equation} \label{eq:categorical_crossentropy}
    L(y,\hat{y})=-\sum\limits_{j=0}^M\sum\limits_{i=0}^N(y_{ij}*log(\hat{y}_{ij}))
\end{equation}

Como se trata de um problema de classificação, escolhemos como função custo do treinamento a função de entropia categórica cruzada, definida na equação \ref{eq:categorical_crossentropy}. Nessa equação, $\hat{y}$ é o vetor das probabilidades das categorias, e $y$ é a categoria real, codificada \textit{one-hot}\footnote{Codificação \textit{one-hot} é uma forma de representar a classe dentre um conjunto de classes à qual um item pertence, na forma de uma sequência binária na qual um único bit, correspondente a esta, esteja no nível lógico 1.}.

Visto que trata-se um problema de classificação binária, seria possível também utilizar a função de entropia cruzada binária; Porém, considerando a possibilidade de expansão futura do modelo para detecção de outras ações conversacionais, tais como gestos com a cabeça, decidimos utilizar uma função custo mais escalável para adição de novas classes.

Como forma de reduzir o \textit{overfitting} da rede neural aos dados do treinamento, aplicamos \textit{Dropout}\footnote{\textit{Dropout} é uma técnica que consiste na remoção aleatória de neurônios temporariamente de uma camada da rede neural durante o treinamento. \textit{Dropout Espacial} é uma variação desta técnica, que consiste em remover neurônios agrupados por região espacial.} às camadas totalmente conectadas desta, e \textit{Dropout Espacial} à saída de suas camadas convolucionais.
A figura \ref{fig:training_metrics} mostra o resultado da utilização dessas técnicas no treinamento do modelo.

\begin{figure}[ht]
    \centering
    \resizebox{0.95\textwidth}{!}{
        \input{figures/train_metrics.tikz}
    }
    \caption{Evolução das métricas de desempenho ao longo dos processos de treinamento e validação do modelo, com e sem \textit{dropout}. Note que, sem \textit{dropout}, a acurácia no conjunto de treinamento tende a 1 e sua função custo tende a 0, enquanto a função custo no conjunto de treinamento não converge. }
    \label{fig:training_metrics}
\end{figure}

Além disso, aplicamos regularização L2 sobre o \kernel\ e \textit{bias} de todas as camadas da rede.
Essa regularização consiste em acrescentar uma penalidade sobre o ajuste do valor de ativação de cada neurônio $x_i$, definida na equação \ref{eq:l2_regularization}, ao valor da função custo definida anteriormente.

\begin{equation}\label{eq:l2_regularization}
    p_{l_2} = l_2 \sum\limits_{i=0}^n(x_i)^2
\end{equation}

Uma vez esgotadas as amostras de teste, o modelo chama, então, o gerador de dados de validação, realizando predições sem retro-propagação sobre as entradas retornadas, e calculando o valor da função custo para os resultados.
Esta etapa é fundamental para garantirmos que o modelo treinado seja aplicável a dados do mundo real, e não exclusivamente aos dados do conjunto de treinamento.

Por fim, ao se esgotarem também essas amostras, dá-se o final de uma época do treinamento, e os geradores tem suas listas de arquivos embaralhadas de forma a garantir aleatoriedade na ordem da próxima época.
Ainda para prevenção de \textit{overfitting} na rede neural foi definido como critério de parada do treinamento a condição de que não haja redução da função custo durante cinquenta épocas do treinamento.

\section{Aplicação}
\label{sec:application}

A aplicação de diarização resultante deste trabalho é capaz de, quando executada sobre um arquivo de vídeo, diarizar o mesmo produzindo um arquivo em formato RTTM\footnote{O formato RTTM, do inglês \textit{Rich Transcription Time Marked}, é um formato de arquivo texto definido pela NIST em 2009 para representação de transcrições. Sua especificação formal pode ser encontrada em \cite{nist2009RT09Rich2009}, publicado pela organização.} identificando os períodos correspondentes à fala do orador. 
Nesta seção discutiremos de maneira aprofundada as considerações feitas durante o processo de desenvolvimento dessa aplicação, assim como os algoritmos implementados. Essa aplicação implementa o algoritmo \ref{alg:application}.

\begin{algorithm}[ht]
    \SetAlgoLined
    \caption{Algoritmo de diarização de mídia.}
    \label{alg:application}
    \KwResult{Diarização completa da mídia de entrada.}
    \While{Mídia de entrada possui quadros}{
        \While{Quadros Carregados < 15}{
            Carrega quadro da mídia de entrada\;
            Aplica pré-processamento sobre o quadro carregado\;
            Adiciona quadro carregado à janela\;
        }
        Realiza predição sobre os quadros da janela utilizando o modelo classificador\;
        Armazena resultado em listas correspondentes aos quadros\;
        Consolida os $step$ quadros mais antigos\;
        Descarta os $step$ quadros mais antigos\;
    }
\end{algorithm}

O algoritmo utilizado para pré-processamento dos quadros já foi discutido na seção \ref{sec:pre-processing}, e as características do modelo de classificação foram discutidas na seção \ref{sec:topology}.
Sendo assim, discutiremos primariamente nesta seção o algoritmo utilizado para consolidação das predições realizadas para os quadros individuais, e ao fim o fluxo de erro adotado quando não é possível obter uma classificação para um determinado quadro.

\subsection{Consolidação das Classificações}
\label{sec:class-commit}

Com a finalidade de obter maior precisão temporal do que seria possível apenas com a classificação de sequências de 15 quadros, a diarização é feita com tamanho de passo $step \leq 15$ tal cada quadro passe pelo classificador um número $n = \floor{15 / step}$ de vezes, em diferentes posições no fragmento.
Dessa forma, torna-se necessário consolidar as diversas predições realizadas sobre cada quadro ao adiciona-los à transcrição final.

Para esse processo, implementamos 5 algoritmos distintos. 
Estes consistem em funções que, quando aplicadas a um conjunto de predições de tamanho variável, retornam um único conjunto de confianças correspondentes a cada classe alvo.
Com esse propósito, implementamos as funções \textit{mediana}, \textit{média}, \textit{frequência de predição}, \textit{média gaussiana}, e \textit{frequência gaussiana}.
A classe de maior confiança retornada por estes algoritmos é considerada a classe real do quadro, e é adicionada à diarização.

\subsubsection{Mediana}

A mediana é o algoritmo de consolidação mais simples entre os algoritmos implementados.
O algoritmo consiste em selecionar as confianças de cada classe referentes à predição realizada com o quadro no centro do fragmento.

\subsubsection{Média}

Este algoritmo consiste em calcular a média aritmética das confianças retornadas pela camada \textit{softmax} do modelo preditor para cada classe.
Essa média é utilizada como nova confiança do quadro.

\subsubsection{Frequência}

O algoritmo de frequência é bastante semelhante ao algoritmo de média, no entanto ao invés de fazer uso das confianças retornadas pelo modelo este considera as predições deste absolutas e seleciona a predição mais frequente para cada quadro individual.

\subsubsection{Média e Frequência Gaussiana}

Estes algoritmos consistem em implementações dos algoritmos de média e frequência já analisados anteriormente nas quais os valores das predições individuais são ponderados a partir de pesos obtidos a partir dos valores de uma distribuição normal (equação \ref{eq:normal-dist}) em $n$ pontos distribuídos linearmente em $[-2, 2]$.

\begin{equation}\label{eq:normal-dist}
    \varphi = \frac{1}{\sqrt{2\pi}} e ^ {\frac{-x^2}{2}}
\end{equation}

A intuição por trás da inclusão destes algoritmos foi de que predições realizadas com o quadro em questão próximo ao centro do fragmento seriam mais representativas da real classe desse do que predições realizadas com o mesmo nas extremidades do fragmento.
O uso de uma distribuição normal, então, enfatizaria as confianças ou frequências dessas classificações.

\subsection{Fluxo de Erro e Dados Ausentes}
\label{sec:missing-data}

Caracterizamos como dado ausente todo quadros no qual o algoritmo de detecção facial não é capaz de detectar um rosto na imagem.
Isso pode ocorrer em diversas situações, desde um movimento por parte do orador que esconda sua face até sua real ausência na gravação.
Note que neste momento não consideramos detecções incorretas como um dado ausente.

Uma vez que o diarizador encontra um dado ausente, torna-se impossível completar uma sequência de 15 quadros para a realização de novas classificações.
É necessário, então, consolidar as predições quanto a todos os quadros já feitas anteriormente.
Isso é feito utilizando os mesmos algoritmos já discutidos na seção \ref{sec:class-commit}.

O quadro que iniciou o erro é, então adicionado à diarização com classe desconhecida.
Quadros subsequentes que também não possuam faces identificáveis incrementam a duração deste trecho de diarização desconhecida.
Uma vez carregados consecutivamente 15 novos quadros válidos, essa é encerrada e o diarizador retorna ao fluxo normal.

Adicionalmente, é também considerado com erro qualquer quadro que, ao ter suas classificações consolidadas, apresente confianças iguais para duas ou mais classes.