\chapter{Revisão Bibliográfica}

\section{Trabalhos Correlatos}
\label{sec:related-work}

O problema de diarização de locutor é historicamente abordado de duas maneiras distintas.
A abordagem tradicional envolve a extração de vetores de características do áudio que se deseja diarizar, podendo ser estes \textit{I-Vectors} \cite{dehakFrontEndFactorAnalysis2011}, extraídos por meio de transformações matriciais aplicadas sobre o sinal, ou \textit{X-Vectors} \cite{snyderXVectorsRobustDNN2018}, obtidos através do uso de redes neurais profundas.
A partir de então, estes vetores podem ser clusterizados \cite{sellSpeakerDiarizationPlda2014} quando informações referentes ao número de locutores é conhecido, ou, mais recentemente, utilizados como entrada de uma rede neural LSTM \cite{wangSpeakerDiarizationLSTM2018}.

No entanto, a introdução das redes neurais convolucionais para reconhecimento de ações humanas em sinais de vídeos \cite{ji3DConvolutionalNeural2013, karpathyLargeScaleVideoClassification2014} implicou na concepção de uma nova abordagem.
Nesta, com o objetivo de melhorar a diarização de sinais heterogêneos, é utilizado um algoritmo de reconhecimento de ações também sobre o sinal de vídeo \cite{hersheyAudiovisualGraphicalModels2004}, que se supõe estar sincronizado ao áudio correspondente.
O reconhecimento de ações é, nesse caso, utilizado para identificar a fala do locutor e, quando combinado com os sinais de áudio, é capaz de produzir resultados melhores do que o estado da arte no processamento exclusivo de áudio \cite{ephratLookingListenCocktail2018}.

\section{Fundamentação Teórica}

Nesta seção discutiremos a fundamentação teórica das várias tecnologias utilizadas na execução do trabalho. Primeiramente, na seção \ref{sec:ann}, apresentaremos o conceito de redes neurais artificiais, utilizadas para construção do identificador de fala. Depois, na seção \ref{sec:facialrecog}, discutiremos a técnica de \textit{histograma de gradientes orientados}, utilizada pelo identificador frontal de rostos. Por fim, na seção \ref{sec:faciallm}, apresentaremos o conceito de floresta de regressão, utilizado para a detecção de marcadores faciais.

\subsection{Redes Neurais Artificiais}
\label{sec:ann}

Redes Neurais Artificiais são um conjunto de algoritmos inspirados pelo funcionamento do cérebro humano, introduzidos pela primeira vez em 1943 quando Warren McCulloch e Walter Pitts modelaram o funcionamento de neurônios através de circuitos elétricos simples\cite{mccullochLogicalCalculusIdeas1943}. 
Esse modelo continuou evoluindo ao longo dos anos, culminando no desenvolvimento do \textit{Perceptron} por Rosenblatt em 1958 \cite{rosenblattPerceptronProbabilisticModel1958} (figura \ref{fig:perceptron}), um algoritmo de classificação binária semelhante a uma rede neural composta por um único neurônio.

No entanto, devido à falta de capacidade computacional e ao grande volume de dados necessários para o treinamento de redes neurais, estes algoritmos permaneceram apenas um conceito acadêmico durante vários anos.
Somente em 2006, Geoffrey Hinton proporia formalmente o conceito de rede neural profunda \cite{hintonFastLearningAlgorithm2006}, um tipo de rede neural composto por múltiplas camadas de neurônios totalmente conectadas.

\begin{figure}[ht]
    \centering
    \input{figures/dnn.tikz}
    \caption{Ilustração de uma rede neural profunda com 3 camadas.}
    \label{fig:dnn}
\end{figure}

Recentemente, com o desenvolvimento de algoritmos para o treinamento de redes neurais em GPU (do Inglês \textit{Graphics Processing Unit}), que possibilitaram o treinamento mais rápido e eficiente de redes neurais de múltiplas camadas, assim como o crescente volume de dados disponível para treinamento, o tópico de redes neurais vem rapidamente ganhando popularidade.
Algoritmos desse tipo já são utilizados em diversas áreas da ciência, incluindo aplicações em medicina \cite{hannunCardiologistLevelArrhythmiaDetection2019,phamPredictingHealthcareTrajectories2017,houDeepSFDeepConvolutional2018}, previsão do tempo \cite{akramSequenceSequenceWeather2016}, veículos autônomos \cite{bojarskiEndEndLearning2016,peddagollaLaneDetectionAutonomous}, entre outras \cite{abiodunStateoftheartArtificialNeural2018}.

Tratam-se de algoritmos próprios para problemas que envolvem o reconhecimento de padrões, e a capacidade de treina-los a partir de dados existentes os torna eficazes para aplicação em tópicos nos quais as relações entre os dados de entrada não sejam totalmente conhecidas.

\subsubsection{Neurônios Artificiais}

Biologicamente, quando o potencial elétrico na base do axônio de um neurônio chega a um valor pré-determinado, por meio do acúmulo de sinais de entrada recebidos pelos dendritos, um pulso elétrico é gerado e propagado até as sinapses, terminais nos quais o neurônio conecta com os demais.
Trata-se de um sinal binário; o neurônio pode ou não estar ativado a cada instante. No entanto, a frequência relativa das ativações pode conter informação \cite{behnkeHierarchicalNeuralNetworks2003}.

\begin{figure}[H]
    \centering
    \includesvg[width=0.7\textwidth]{neurono-ido.svg}
    \caption{A estrutura de um neurônio biológico. Imagem adaptada de \cite{dhp1080IdoSkemoPri2016}.}
    \label{fig:bio_neuron}
\end{figure}

Funções de Ativação, também conhecidas como funções de propagação ou neurônios artificiais quando aplicadas no contexto de redes neurais, buscam modelar de forma simplificada esses aspectos biológicos.
Elas recebem como entrada os valores resultantes das camadas anteriores, ponderados por pesos específicos a cada conexão, e, quando estes valores atingem um limiar de excitação pré-determinado, os transformam em uma única saída.

\begin{figure}[H]
    \centering
    \input{figures/perceptron.tikz}
    \caption{O \textit{Perceptron}, uma rede neural com um único neurônio.}
    \label{fig:perceptron}
\end{figure}

Geralmente, a função de ativação é constante e independente do processo de treinamento da rede neural; o ajuste é feito sobre os pesos das conexões, através algoritmo de retropropagação \cite{dreyfusArtificialNeuralNetworks1990}.
O algoritmo é capaz de, a partir do resultado de uma camada, calcular o efeito de cada peso sobre o gradiente da função.
Assim, ele possibilita a utilização de algoritmos tradicionais de otimização para encontrar os pesos locais de cada conexão, mesmo em redes com múltiplas camadas de centenas de neurônios, em função do efeito global destes.

Comumente, são utilizadas como função de ativação as funções ReLU (\textit{Rectified Linear Unit}), tangente hiperbólica, sigmoide, e Heaviside, em função do intervalo de saída e comportamento desejado.

\begin{equation}\label{eq:relu}
    \phi(x) = \left\{
        \begin{array}{ll}
            0, & x \le 0\\
            x, & x > 0
        \end{array}
    \right.
\end{equation}

A função ReLU (equação \ref{eq:relu}), utilizada neste trabalho, tem comportamento tal que sua ativação ocorre sempre que $x>0$, com o valor resultante sendo o próprio valor de $x$. 
Essa função é ideal para aplicações em redes neurais profundas pois o gradiente é preservado após cada camada da rede, o que não ocorre com as funções sigmoide e tangente hiperbólica.
Nessas, o gradiente tende a valores infinitesimalmente pequenos após camadas sucessivas, dificultando o uso do algoritmo de retropropagação.
Além disso, o treinamento de redes neurais profundas que utilizam a função ReLU é mais rápido do que o das outras funções de ativação, devido à sua simplicidade \cite{krizhevskyImageNetClassificationDeep2017}.

\subsubsection{Redes Neurais Convolucionais}
\label{sec:convnet}

Redes Neurais Convolucionais são redes neurais compostas por uma ou mais camadas de convolução.
São ideais para o tratamento de dados matriciais, como por exemplo imagens, que constituem em uma matriz bi-dimensional de pixels, ou vídeos, matrizes tridimensionais de imagens.
Essas redes são capazes de identificar relações entre os elementos adjacentes de uma matriz, e, através da técnica de Max Pooling, focar somente nos pontos mais relevantes, mesmo em conjuntos de dados muito grandes.

\paragraph{Convolução e Filtros}
\label{sec:convolution}

A convolução é uma técnica matemática que consiste na aplicação de uma matriz, denominada matriz de convolução ou \textit{filtro} quando utilizada no contexto de processamento de imagens, sobre cada elemento de uma outra matriz.
Esse processo produz uma nova matriz na qual o valor de cada elemento corresponde ao seu valor e o valor de seus vizinhos, ponderados pelos valores da matriz de convolução, denominados \kernel\ da convolução.

Para dados bi-dimensionais, a operação de convolução pode ser descrita da seguinte forma:

\begin{equation}
    g(x,y) = \omega * f(x,y) = \sum\limits_{s=-a}^a\sum\limits_{t=-b}^b\omega(s,t)f(x-s, y-t)
\end{equation}

Essa técnica é frequentemente utilizada em áreas que trabalham com imagens, como na área de veículos autônomos, onde é capaz de identificar as faixas pintadas sobre o asfalto \cite{peddagollaLaneDetectionAutonomous}, assim como outros veículos e pedestres na rua.
Com a aplicação dessa, é possível remover características da imagem de entrada irrelevantes para o problema que se deseja resolver, como mostra a figura \ref{fig:conv}, diminuindo a complexidade do problema a cada camada.

\begin{figure}[ht]
    \centering
    \input{figures/convolution.tikz}
    \caption{O resultado da aplicação de um \kernel\ simples para detecção de bordas verticais a uma foto. 
    O \kernel\ é aplicado a cada pixel da imagem, substituindo o valor deste pela soma de sua vizinhança ponderada pelos valores da matriz. 
    Foto original obtida de \cite{diliffEnglishLookingEast2015}.}
    \label{fig:conv}
\end{figure}

No contexto de redes neurais convolucionais, isso consiste em neurônios que realizam uma operação de convolução sobre os dados de entrada.
O \kernel\ da matriz de convolução é treinado de forma a identificar os aspectos da entrada mais relevantes para o problema, através do algoritmo de retropropagação.

\paragraph{Pooling}
\label{sec:pooling}

Um outro tipo de camada tradicional nas redes neurais convolucionais é a camada de \textit{pooling}, ou ``agrupamento''.
Esse tipo de camada tem como função reduzir o número e complexidade dos dados de entrada e promover uma melhor generalização dos dados espaciais.
Isso é necessários pois as camadas convolucionais tendem a manter a localização espacial das características detectadas na matriz de entrada, informação que na maioria dos casos não é relevante para o problema que se deseja resolver.

A operação consiste em subdividir a matriz de entrada em conjuntos de valores e, segundo um critério especificado, escolher entre estes um único valor resultante.

\begin{figure}[ht]
    \centering
    \input{figures/max_pool.tikz}
    \caption{O resultado da operação de \textit{Max Pooling} com dimensão $2\times2$ sobre uma matriz de entrada.}
    \label{fig:maxpool}
\end{figure}

No contexto de redes neurais convolucionais, no entanto, dado que os valores resultantes da camada convolucional teriam sido filtrados por uma função de ativação não-linear, como a ReLU (equação \ref{eq:relu}), temos que os valores obtidos correspondem à intensidade da ativação dos neurônios.
Assim, devido ao fato de que valores maiores devem corresponder a características de maior importância na matriz de entrada, temos dois critérios mais usados para a operação:

\begin{description}
    \itemsep0em 
    \item \textit{\textbf{Max Pooling}}: Para cada grupo, calcula o valor máximo (figura \ref{fig:maxpool}).
    \item \textit{\textbf{Average Pooling}}: Para cada grupo, calcula a média dos valores.
\end{description}

A escolha entre esses critérios é feita em função do propósito da camada anterior; se esta for, por exemplo, um detector de bordas em imagens, temos que a técnica de \textit{Max Pooling} será preferível, visto que as bordas serão sempre o maior valor do grupo, enquanto os demais elementos terão valor zero.
Já no caso de se tratar de um reconhecedor de variação temporal, a técnica de \textit{Average Pooling} pode ser preferível, já que será capaz de manter informações sobre o valor médio dos elementos no instante que está sendo comprimido.

\subsection{Histograma de Gradientes Orientados}
\label{sec:facialrecog}

Histograma de Gradientes Orientados ou HOG, do inglês \textit{Histogram of Oriented Gradients}, é uma técnica de visão computacional utilizada para detecção de objetos em imagens, introduzida pela primeira vez em 1982 por Robert McConnel \cite{mcconnellMethodApparatusPattern1986}, e formalizada em 1994 por Freeman e Roth \cite{freemanOrientationHistogramsHand}.
Trata-se do treinamento de um histograma de blocos descritivos dos gradientes que se espera observar em cada um dos pontos que se deseja conhecer, e posterior comparação deste com os gradientes de áreas ou células da imagem de entrada.

A técnica consiste em, primeiramente, calcular os gradientes de cada ponto da imagem, normalizada quanto à cor, frequentemente por sua conversão em escala de cinza, a partir da aplicação de um vetor de diferenciação $[-1, 0, 1]$ a cada pixel desta, em ambas as direções \cite{dalalHistogramsOrientedGradients2005}.
Esse processo calcula o gradiente local de cada um dos pontos a que são aplicados.

Uma vez calculados os gradientes em todos os pontos da figura, estes são agrupados em regiões espaciais e passam a compor as células do histograma, no processo chamado de ``agrupamento espacial'' (\textit{spatial/orientation binning}).
A orientação final de cada bloco é, então, calculada a partir dos gradientes desta, através de uma média ponderada de suas orientações.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.30\textwidth]{face_fhog_filters.png}
    \caption{Um exemplo de detector HOG para identificação facial frontal. Imagem retirada de \cite{kingDlib18Released}.}
    \label{fig:dlib_hog}
\end{figure}

Por fim, é feita a normalização dos blocos descritores. % TODO: Continue writing this section.

\subsection{Detecção de Marcadores Faciais}
\label{sec:faciallm}