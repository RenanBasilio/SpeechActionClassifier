
@article{barbosaSyllabletimingBrazilianPortuguese2000,
  title = {"{{Syllable}}-Timing in {{Brazilian Portuguese}}": Uma Cr{\'i}tica a {{Roy Major}}},
  shorttitle = {"{{Syllable}}-Timing in {{Brazilian Portuguese}}"},
  author = {Barbosa, Pl{\'i}nio Almeida},
  year = {2000},
  volume = {16},
  pages = {369--402},
  issn = {0102-4450},
  doi = {10.1590/S0102-44502000000200006},
  abstract = {A tese de R. Major, segundo a qual haveria evid{\^e}ncias para se considerar o portugu{\^e}s brasileiro (PB) como "stress-timing" ou tendendo para tal, {\'e} rediscutida. As quest{\~o}es fon{\'e}tico-fonol{\'o}gicas suscitadas pela dicotomia de l{\'i}nguas "stress-timed" e "syllable-timed" e o suposto isocronismo absoluto s{\~a}o apresentadas sob um prisma estritamente pros{\'o}dico-temporal. Um modelo empregando dois osciladores acoplados (acentual e sil{\'a}bico) possibilita a caracteriza{\c c}{\~a}o biparam{\'e}trica (taxa de elocu{\c c}{\~a}o e for{\c c}a de acoplamento) de um conjunto arbitr{\'a}rio de frases de uma l{\'i}ngua e permite mostrar que, em PB, h{\'a} alto grau de "syllable-timing". {\`A} luz de uma an{\'a}lise fon{\'e}tica mais cuidadosa dos fatores ligados ao ritmo, mostra-se que os argumentos apresentados por Major para justificar "stress-timing" em PB s{\~a}o completamente equivocados.
          , 
            This paper reintroduces the discussion about stress-timing in Brazilian Portuguese (BP). It begins by surveying some phonetic and phonological issues raised by the syllable- vs stress-timed dichotomy which culminated with the emergence of the p-center notion. Strict considerations of timing of V-V units and stress groups are taken into account to analyze the long term coupling of two basic oscillators (vowel and stress flow). This coupling allows a two-parameter characterization of language rhythms (coupling strength and speech rate) revealing that BP utterances present a high-degree of syllable-timing. A comparison with other languages, including European Portuguese, is also presented. The results analyzed indicate that Major's arguments for considering Portuguese (sic) as stress-timing are misleading.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\95Y6SX5G\\Barbosa - 2000 - Syllable-timing in Brazilian Portuguese uma cr√≠.pdf},
  journal = {DELTA: Documenta{\c c}{\~a}o de Estudos em Ling{\"u}{\'i}stica Te{\'o}rica e Aplicada},
  number = {2}
}

@inproceedings{dalalHistogramsOrientedGradients2005,
  title = {Histograms of {{Oriented Gradients}} for {{Human Detection}}},
  booktitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  author = {Dalal, N. and Triggs, B.},
  year = {2005},
  volume = {1},
  pages = {886--893},
  publisher = {{IEEE}},
  address = {{San Diego, CA, USA}},
  doi = {10.1109/CVPR.2005.177},
  abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\7I8IHMMA\\Dalal and Triggs - 2005 - Histograms of Oriented Gradients for Human Detecti.pdf},
  isbn = {978-0-7695-2372-9},
  language = {en}
}

@article{dimitriadisEnhancementsAudioonlyDiarization2019,
  title = {Enhancements for {{Audio}}-Only {{Diarization Systems}}},
  author = {Dimitriadis, Dimitrios},
  year = {2019},
  month = aug,
  url = {http://arxiv.org/abs/1909.00082},
  urldate = {2020-02-08},
  abstract = {In this paper two different approaches to enhance the performance of the most challenging component of a Speaker Diarization system are presented, i.e. the speaker clustering part. A processing step is proposed enhancing the input features with a temporal smoothing process combined with nonlinear filtering. We, also, propose improvements on the Deep Embedded Clustering (DEC) algorithm -- a nonlinear feature transformation. The performance of these enhancements is compared with different clustering algorithms, such as the UISRNN, k-Means, Spectral clustering and x-Means. The evaluation is held on three different tasks, i.e. the AMI, DIHARD and an internal meeting transcription task. The proposed approaches assume a known number of speakers and time segmentations for the audio files. Since, we focus only on the clustering component of diarization for this work, the segmentation provided is assumed perfect. Finally, we present how supervision, in the form of given speaker profiles, can further improve the overall diarization performance. The proposed enhancements yield substantial relative improvements in all 3 tasks, with 20\textbackslash\% in AMI and 19\textbackslash\% better than the best diarization system for DIHARD task, when the number of speakers is known.},
  archivePrefix = {arXiv},
  eprint = {1909.00082},
  eprinttype = {arxiv},
  file = {C\:\\Users\\renan\\Zotero\\storage\\8P4SLIKC\\Dimitriadis - 2019 - Enhancements for Audio-only Diarization Systems.pdf;C\:\\Users\\renan\\Zotero\\storage\\WUVWNVCG\\1909.html},
  journal = {arXiv:1909.00082 [cs, eess]},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{dlib09,
  title = {Dlib-Ml: {{A}} Machine Learning Toolkit},
  author = {King, Davis E.},
  year = {2009},
  volume = {10},
  pages = {1755--1758},
  journal = {Journal of Machine Learning Research}
}

@article{hintonFastLearningAlgorithm2006,
  title = {A {{Fast Learning Algorithm}} for {{Deep Belief Nets}}},
  author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
  year = {2006},
  month = jul,
  volume = {18},
  pages = {1527--1554},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2006.18.7.1527},
  file = {C\:\\Users\\renan\\Zotero\\storage\\RFBGEQJN\\Hinton et al. - 2006 - A Fast Learning Algorithm for Deep Belief Nets.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {7}
}

@article{Hunter:2007,
  title = {Matplotlib: {{A 2D}} Graphics Environment},
  author = {Hunter, J. D.},
  year = {2007},
  volume = {9},
  pages = {90--95},
  publisher = {{IEEE COMPUTER SOC}},
  doi = {10.1109/MCSE.2007.55},
  abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.},
  journal = {Computing in Science \& Engineering},
  number = {3}
}

@inproceedings{kazemiOneMillisecondFace2014,
  title = {One Millisecond Face Alignment with an Ensemble of Regression Trees},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Kazemi, Vahid and Sullivan, Josephine},
  year = {2014},
  month = jun,
  pages = {1867--1874},
  publisher = {{IEEE}},
  address = {{Columbus, OH}},
  doi = {10.1109/CVPR.2014.241},
  abstract = {This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face's landmark positions directly from a sparse subset of pixel intensities, achieving super-realtime performance with high quality predictions. We present a general framework based on gradient boosting for learning an ensemble of regression trees that optimizes the sum of square error loss and naturally handles missing or partially labelled data. We show how using appropriate priors exploiting the structure of image data helps with efficient feature selection. Different regularization strategies and its importance to combat overfitting are also investigated. In addition, we analyse the effect of the quantity of training data on the accuracy of the predictions and explore the effect of data augmentation using synthesized data.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\7CIFBZ8V\\Kazemi and Sullivan - 2014 - One millisecond face alignment with an ensemble of.pdf},
  isbn = {978-1-4799-5118-5},
  language = {en}
}

@inproceedings{Kluyver:2016aa,
  title = {Jupyter {{Notebooks}} \textendash{} a Publishing Format for Reproducible Computational Workflows},
  booktitle = {Positioning and Power in Academic Publishing: {{Players}}, Agents and Agendas},
  author = {Kluyver, Thomas and {Ragan-Kelley}, Benjamin and P{\'e}rez, Fernando and Granger, Brian and Bussonnier, Matthias and Frederic, Jonathan and Kelley, Kyle and Hamrick, Jessica and Grout, Jason and Corlay, Sylvain and Ivanov, Paul and Avila, Dami{\'a}n and Abdalla, Safia and Willing, Carol},
  editor = {Loizides, F. and Schmidt, B.},
  year = {2016},
  pages = {87--90},
  organization = {{IOS Press}}
}

@inproceedings{lelanSpeakerDiarizationUnsupervised2016,
  title = {Speaker {{Diarization With Unsupervised Training Framework}}},
  booktitle = {41st {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}} 2016)},
  author = {Le Lan, Ga{\"e}l and Meignier, Sylvain and Charlet, Delphine and Del{\'e}glise, Paul},
  year = {2016},
  month = mar,
  pages = {5},
  address = {{Shanghai, China}},
  doi = {10.1109/ICASSP.2016.7472741},
  abstract = {This paper investigates single and cross-show diarization based on an unsupervised i-vector framework, on French TV and Radio corpora. This framework uses speaker clustering as a way to automatically select data from unlabeled corpora to train i-vector PLDA models. Performances between supervised and unsupervised models are compared. The experimental results on two distinct test corpora (one TV, one Radio) show that unsupervised models perform as good as supervised models for both tasks. Such results indicate that performing an effective cross-show diarization on new language or new domain data in the future should not depend on the availability of manually annotated data.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\WDAI2TCA\\Le Lan et al. - 2016 - Speaker Diarization With Unsupervised Training Fra.pdf}
}

@inproceedings{mccowanAMIMeetingCorpus2005,
  title = {The {{AMI Meeting Corpus}}},
  booktitle = {In: {{Proceedings Measuring Behavior}} 2005, 5th {{International Conference}} on {{Methods}} and {{Techniques}} in {{Behavioral Research}}. {{L}}.{{P}}.{{J}}.{{J}}. {{Noldus}}, {{F}}. {{Grieco}}, {{L}}.{{W}}.{{S}}. {{Loijens}} and {{P}}.{{H}}. {{Zimmerman}} ({{Eds}}.), {{Wageningen}}: {{Noldus Information Technology}}},
  author = {Mccowan, I. and Lathoud, G. and Lincoln, M. and Lisowska, A. and Post, W. and Reidsma, D. and Wellner, P.},
  year = {2005},
  abstract = {To support multi-disciplinary research in the AMI (Augmented Multi-party Interaction) project, a 100 hour corpus of meetings is being collected. This corpus is being recorded in several instrumented rooms equipped with a variety of microphones, video cameras, electronic pens, presentation slide capture and white-board capture devices. As well as real meetings, the corpus contains a significant proportion of scenario-driven meetings, which have been designed to elicit a rich range of realistic behaviors. To facilitate research, the raw data are being annotated at a number of levels including speech transcriptions, dialogue acts and summaries. The corpus is being distributed using a web server designed to allow convenient browsing and download of multimedia content and associated annotations. This article first overviews AMI research themes, then discusses corpus design, as well as data collection, annotation and distribution.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\W3JCAPJK\\Mccowan et al. - 2005 - The AMI Meeting Corpus.pdf;C\:\\Users\\renan\\Zotero\\storage\\FHP28W4E\\summary.html}
}

@article{mccullochLogicalCalculusIdeas1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S. and Pitts, Walter},
  year = {1943},
  month = dec,
  volume = {5},
  pages = {115--133},
  issn = {1522-9602},
  doi = {10.1007/BF02478259},
  abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\TQZHHSCE\\McCulloch and Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf},
  journal = {The bulletin of mathematical biophysics},
  language = {en},
  number = {4}
}

@inproceedings{mckinney-proc-scipy-2010,
  title = {Data {{Structures}} for {{Statistical Computing}} in {{Python}}},
  booktitle = {Proceedings of the 9th {{Python}} in {{Science Conference}}},
  author = {{Wes McKinney}},
  editor = {{van der Walt}, St{\'e}fan and {Jarrod Millman}},
  year = {2010},
  pages = {51--56}
}

@misc{mtheilerDeteccaoMarcadoresFaciais2019,
  title = {Detec{\c c}{\~a}o de {{Marcadores Faciais}} Pela Biblioteca {{Dlib}}},
  shorttitle = {English},
  author = {{MTheiler}},
  year = {2019},
  month = jan,
  url = {https://commons.wikimedia.org/wiki/File:Dlib-face_landmark_detection.jpg},
  urldate = {2020-02-25},
  file = {C\:\\Users\\renan\\Zotero\\storage\\M34H965E\\FileDlib-face_landmark_detection.html}
}

@article{opencv_library,
  title = {The {{OpenCV}} Library},
  author = {Bradski, G.},
  year = {2000},
  citeulike-article-id = {2236121},
  journal = {Dr. Dobb's Journal of Software Tools},
  keywords = {bibtex-import},
  posted-at = {2008-01-15 19:21:54},
  priority = {4}
}

@article{oxenhamPitchPerception2012,
  title = {Pitch {{Perception}}},
  author = {Oxenham, Andrew J.},
  year = {2012},
  month = sep,
  volume = {32},
  pages = {13335--13338},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3815-12.2012},
  abstract = {Pitch is one of the primary auditory sensations and plays a defining role in music, speech, and auditory scene analysis. Although the main physical correlate of pitch is acoustic periodicity, or repetition rate, there are many interactions that complicate the relationship between the physical stimulus and the perception of pitch. In particular, the effects of other acoustic parameters on pitch judgments, and the complex interactions between perceptual organization and pitch, have uncovered interesting perceptual phenomena that should help to reveal the underlying neural mechanisms.},
  copyright = {Copyright \textcopyright{} 2012 the authors 0270-6474/12/3213335-04\$15.00/0},
  file = {C\:\\Users\\renan\\Zotero\\storage\\4UP3WFTR\\Oxenham - 2012 - Pitch Perception.pdf;C\:\\Users\\renan\\Zotero\\storage\\GIDP5F5Y\\13335.html},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {39},
  pmid = {23015422}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The {{Perceptron}}: {{A Probabilistic Model}} for {{Information Storage}} and {{Organization}} in {{The Brain}}},
  shorttitle = {The {{Perceptron}}},
  author = {Rosenblatt, F.},
  year = {1958},
  pages = {65--386},
  abstract = {If we are eventually to understand the capability of higher organisms for perceptual recognition, generalization, recall, and thinking, we must first have answers to three fundamental questions: 1. How is information about the physical world sensed, or detected, by the biological system? 2. In what form is information stored, or remembered? 3. How does information contained in storage, or in memory, influence recognition and behavior? The first of these questions is in the},
  file = {C\:\\Users\\renan\\Zotero\\storage\\85I9GKY8\\Rosenblatt - 1958 - The Perceptron A Probabilistic Model for Informat.pdf;C\:\\Users\\renan\\Zotero\\storage\\RHY68RX7\\summary.html},
  journal = {Psychological Review}
}

@inproceedings{snyderXVectorsRobustDNN2018,
  title = {X-{{Vectors}}: {{Robust DNN Embeddings}} for {{Speaker Recognition}}},
  shorttitle = {X-{{Vectors}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Snyder, David and {Garcia-Romero}, Daniel and Sell, Gregory and Povey, Daniel and Khudanpur, Sanjeev},
  year = {2018},
  month = apr,
  pages = {5329--5333},
  publisher = {{IEEE}},
  address = {{Calgary, AB}},
  doi = {10.1109/ICASSP.2018.8461375},
  abstract = {In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\95UVGLQV\\Snyder et al. - 2018 - X-Vectors Robust DNN Embeddings for Speaker Recog.pdf},
  isbn = {978-1-5386-4658-8},
  language = {en}
}

@article{tensorflow2015-whitepaper,
  title = {{{TensorFlow}}: {{Large}}-Scale {{Machine Learning}} on {{Heterogeneous Systems}}},
  author = {Abadi, Mart{\'i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'e}, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'e}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2015},
  url = {https://www.tensorflow.org/},
  abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, generalpurpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous ``parameter server'' designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.}
}

@book{werbosRegressionNewTools1975,
  title = {Beyond {{Regression}}: {{New Tools}} for {{Prediction}} and {{Analysis}} in the {{Behavioral Sciences}}},
  shorttitle = {Beyond {{Regression}}},
  author = {Werbos, Paul John},
  year = {1975},
  publisher = {{Harvard University}},
  googlebooks = {z81XmgEACAAJ},
  language = {en}
}

@article{zewoudieUseLongtermFeatures2018,
  title = {The Use of Long-Term Features for {{GMM}}- and i-Vector-Based Speaker Diarization Systems},
  author = {Zewoudie, Abraham Woubie and Luque, Jordi and Hernando, Javier},
  year = {2018},
  month = sep,
  volume = {2018},
  pages = {14},
  issn = {1687-4722},
  doi = {10.1186/s13636-018-0140-x},
  abstract = {Several factors contribute to the performance of speaker diarization systems. For instance, the appropriate selection of speech features is one of the key aspects that affect speaker diarization systems. The other factors include the techniques employed to perform both segmentation and clustering. While the static mel frequency cepstral coefficients are the most widely used features in speech-related tasks including speaker diarization, several studies have shown the benefits of augmenting regular speech features with the static ones.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\GXP27EB6\\Zewoudie et al. - 2018 - The use of long-term features for GMM- and i-vecto.pdf},
  journal = {EURASIP Journal on Audio, Speech, and Music Processing},
  language = {en},
  number = {1}
}


