
@article{barbosaSyllabletimingBrazilianPortuguese2000,
  title = {"{{Syllable}}-Timing in {{Brazilian Portuguese}}": Uma Cr{\'i}tica a {{Roy Major}}},
  shorttitle = {"{{Syllable}}-Timing in {{Brazilian Portuguese}}"},
  author = {Barbosa, Pl{\'i}nio Almeida},
  year = {2000},
  volume = {16},
  pages = {369--402},
  issn = {0102-4450},
  doi = {10.1590/S0102-44502000000200006},
  abstract = {A tese de R. Major, segundo a qual haveria evid{\^e}ncias para se considerar o portugu{\^e}s brasileiro (PB) como "stress-timing" ou tendendo para tal, {\'e} rediscutida. As quest{\~o}es fon{\'e}tico-fonol{\'o}gicas suscitadas pela dicotomia de l{\'i}nguas "stress-timed" e "syllable-timed" e o suposto isocronismo absoluto s{\~a}o apresentadas sob um prisma estritamente pros{\'o}dico-temporal. Um modelo empregando dois osciladores acoplados (acentual e sil{\'a}bico) possibilita a caracteriza{\c c}{\~a}o biparam{\'e}trica (taxa de elocu{\c c}{\~a}o e for{\c c}a de acoplamento) de um conjunto arbitr{\'a}rio de frases de uma l{\'i}ngua e permite mostrar que, em PB, h{\'a} alto grau de "syllable-timing". {\`A} luz de uma an{\'a}lise fon{\'e}tica mais cuidadosa dos fatores ligados ao ritmo, mostra-se que os argumentos apresentados por Major para justificar "stress-timing" em PB s{\~a}o completamente equivocados.
          , 
            This paper reintroduces the discussion about stress-timing in Brazilian Portuguese (BP). It begins by surveying some phonetic and phonological issues raised by the syllable- vs stress-timed dichotomy which culminated with the emergence of the p-center notion. Strict considerations of timing of V-V units and stress groups are taken into account to analyze the long term coupling of two basic oscillators (vowel and stress flow). This coupling allows a two-parameter characterization of language rhythms (coupling strength and speech rate) revealing that BP utterances present a high-degree of syllable-timing. A comparison with other languages, including European Portuguese, is also presented. The results analyzed indicate that Major's arguments for considering Portuguese (sic) as stress-timing are misleading.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\95Y6SX5G\\Barbosa - 2000 - Syllable-timing in Brazilian Portuguese uma cr√≠.pdf},
  journal = {DELTA: Documenta{\c c}{\~a}o de Estudos em Ling{\"u}{\'i}stica Te{\'o}rica e Aplicada},
  number = {2}
}

@inproceedings{dalalHistogramsOrientedGradients2005,
  title = {Histograms of {{Oriented Gradients}} for {{Human Detection}}},
  booktitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  author = {Dalal, N. and Triggs, B.},
  year = {2005},
  volume = {1},
  pages = {886--893},
  publisher = {{IEEE}},
  address = {{San Diego, CA, USA}},
  doi = {10.1109/CVPR.2005.177},
  abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\7I8IHMMA\\Dalal and Triggs - 2005 - Histograms of Oriented Gradients for Human Detecti.pdf},
  isbn = {978-0-7695-2372-9},
  language = {en}
}

@article{dehakFrontEndFactorAnalysis2011,
  title = {Front-{{End Factor Analysis}} for {{Speaker Verification}}},
  author = {Dehak, Najim and Kenny, Patrick J. and Dehak, R{\'e}da and Dumouchel, Pierre and Ouellet, Pierre},
  year = {2011},
  month = may,
  volume = {19},
  pages = {788--798},
  issn = {1558-7924},
  doi = {10.1109/TASL.2010.2064307},
  abstract = {This paper presents an extension of our previous work which proposes a new speaker representation for speaker verification. In this modeling, a new low-dimensional speaker- and channel-dependent space is defined using a simple factor analysis. This space is named the total variability space because it models both speaker and channel variabilities. Two speaker verification systems are proposed which use this new representation. The first system is a support vector machine-based system that uses the cosine kernel to estimate the similarity between the input data. The second system directly uses the cosine similarity as the final decision score. We tested three channel compensation techniques in the total variability space, which are within-class covariance normalization (WCCN), linear discriminate analysis (LDA), and nuisance attribute projection (NAP). We found that the best results are obtained when LDA is followed by WCCN. We achieved an equal error rate (EER) of 1.12\% and MinDCF of 0.0094 using the cosine distance scoring on the male English trials of the core condition of the NIST 2008 Speaker Recognition Evaluation dataset. We also obtained 4\% absolute EER improvement for both-gender trials on the 10 s-10 s condition compared to the classical joint factor analysis scoring.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\C52WASEX\\Dehak et al. - 2011 - Front-End Factor Analysis for Speaker Verification.pdf;C\:\\Users\\renan\\Zotero\\storage\\2WDMW3E4\\5545402.html},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  keywords = {channel compensation technique,channel dependent space,Context modeling,Cosine distance scoring,cosine kernel,decision score,front end factor analysis,joint factor analysis (JFA),Kernel,Linear discriminant analysis,linear discriminate analysis,low dimensional speaker,Natural languages,NIST,nuisance attribute projection,Permission,similarity estimation,speaker recognition,Speaker recognition,speaker recognition evaluation,speaker representation,speaker verification,Speech analysis,support vector machine,support vector machines,Support vector machines,support vector machines (SVMs),Testing,total variability space,variability space,within class covariance normalization},
  note = {Conference Name: IEEE Transactions on Audio, Speech, and Language Processing},
  number = {4}
}

@article{dimitriadisEnhancementsAudioonlyDiarization2019,
  title = {Enhancements for {{Audio}}-Only {{Diarization Systems}}},
  author = {Dimitriadis, Dimitrios},
  year = {2019},
  month = aug,
  url = {http://arxiv.org/abs/1909.00082},
  urldate = {2020-02-08},
  abstract = {In this paper two different approaches to enhance the performance of the most challenging component of a Speaker Diarization system are presented, i.e. the speaker clustering part. A processing step is proposed enhancing the input features with a temporal smoothing process combined with nonlinear filtering. We, also, propose improvements on the Deep Embedded Clustering (DEC) algorithm -- a nonlinear feature transformation. The performance of these enhancements is compared with different clustering algorithms, such as the UISRNN, k-Means, Spectral clustering and x-Means. The evaluation is held on three different tasks, i.e. the AMI, DIHARD and an internal meeting transcription task. The proposed approaches assume a known number of speakers and time segmentations for the audio files. Since, we focus only on the clustering component of diarization for this work, the segmentation provided is assumed perfect. Finally, we present how supervision, in the form of given speaker profiles, can further improve the overall diarization performance. The proposed enhancements yield substantial relative improvements in all 3 tasks, with 20\textbackslash\% in AMI and 19\textbackslash\% better than the best diarization system for DIHARD task, when the number of speakers is known.},
  archivePrefix = {arXiv},
  eprint = {1909.00082},
  eprinttype = {arxiv},
  file = {C\:\\Users\\renan\\Zotero\\storage\\8P4SLIKC\\Dimitriadis - 2019 - Enhancements for Audio-only Diarization Systems.pdf;C\:\\Users\\renan\\Zotero\\storage\\WUVWNVCG\\1909.html},
  journal = {arXiv:1909.00082 [cs, eess]},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{dlib09,
  title = {Dlib-Ml: {{A}} Machine Learning Toolkit},
  author = {King, Davis E.},
  year = {2009},
  volume = {10},
  pages = {1755--1758},
  journal = {Journal of Machine Learning Research}
}

@article{ephratLookingListenCocktail2018,
  title = {Looking to {{Listen}} at the {{Cocktail Party}}: {{A Speaker}}-{{Independent Audio}}-{{Visual Model}} for {{Speech Separation}}},
  shorttitle = {Looking to {{Listen}} at the {{Cocktail Party}}},
  author = {Ephrat, Ariel and Mosseri, Inbar and Lang, Oran and Dekel, Tali and Wilson, Kevin and Hassidim, Avinatan and Freeman, William T. and Rubinstein, Michael},
  year = {2018},
  month = jul,
  volume = {37},
  pages = {1--11},
  issn = {07300301},
  doi = {10.1145/3197517.3201357},
  abstract = {We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to "focus" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVSpeech, a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).},
  archivePrefix = {arXiv},
  eprint = {1804.03619},
  eprinttype = {arxiv},
  file = {C\:\\Users\\renan\\Zotero\\storage\\658VDDKT\\Ephrat et al. - 2018 - Looking to Listen at the Cocktail Party A Speaker.pdf;C\:\\Users\\renan\\Zotero\\storage\\9WEKHLPB\\1804.html},
  journal = {ACM Transactions on Graphics},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  number = {4}
}

@inproceedings{hersheyAudiovisualGraphicalModels2004,
  title = {Audio-Visual Graphical Models for Speech Processing},
  booktitle = {Acoustics, {{Speech}}, and {{Signal Processing}}, 1988. {{ICASSP}}-88., 1988 {{International Conference}} On},
  author = {Hershey, John and Attias, H. and Jojic, Nebojsa and Kristjansson, Trausti},
  year = {2004},
  month = jun,
  volume = {5},
  pages = {V-649},
  doi = {10.1109/ICASSP.2004.1327194},
  abstract = {Perceiving sounds in a noisy environment is a challenging problem. Visual lip-reading can provide relevant information but is also challenging because lips are moving and a tracker must deal with a variety of conditions. Typically audio-visual systems have been assembled from individually engineered modules. We propose to fuse audio and video in a probabilistic generative model that implements cross-model self-supervised learning, enabling adaptation to audio-visual data. The video model features a Gaussian mixture model embedded in a linear subspace of a sprite which translates in the video. The system can learn to detect and enhance speech in noise given only a short (30 second) sequence of audio-visual data. We show some results for speech detection and enhancement, and discuss extensions to the model that are under investigation.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\NB5IPHQA\\Hershey et al. - 2004 - Audio-visual graphical models for speech processin.pdf}
}

@article{hintonFastLearningAlgorithm2006,
  title = {A {{Fast Learning Algorithm}} for {{Deep Belief Nets}}},
  author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
  year = {2006},
  month = jul,
  volume = {18},
  pages = {1527--1554},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2006.18.7.1527},
  file = {C\:\\Users\\renan\\Zotero\\storage\\RFBGEQJN\\Hinton et al. - 2006 - A Fast Learning Algorithm for Deep Belief Nets.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {7}
}

@article{hunterMatplotlib2DGraphics2007,
  title = {Matplotlib: {{A 2D Graphics Environment}}},
  shorttitle = {Matplotlib},
  author = {Hunter, John D.},
  year = {2007},
  month = may,
  volume = {9},
  pages = {90--95},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2007.55},
  abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting,and publication-quality image generation across user interfaces and operating systems},
  file = {C\:\\Users\\renan\\Zotero\\storage\\5Q4ITRYD\\Hunter - 2007 - Matplotlib A 2D Graphics Environment.pdf;C\:\\Users\\renan\\Zotero\\storage\\3P3S2ZSI\\4160265.html},
  journal = {Computing in Science Engineering},
  keywords = {2D graphics package,application development,computer graphics,Computer languages,Equations,Graphical user interfaces,Graphics,Image generation,interactive scripting,Interpolation,mathematics computing,Matplotlib,object-oriented programming,operating system,Operating systems,Packaging,Programming profession,publication-quality image generation,Python,scientific programming,scripting languages,software packages,user interface,User interfaces},
  note = {Conference Name: Computing in Science Engineering},
  number = {3}
}

@article{ji3DConvolutionalNeural2013,
  title = {{{3D Convolutional Neural Networks}} for {{Human Action Recognition}}},
  author = {Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
  year = {2013},
  month = jan,
  volume = {35},
  pages = {221--231},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2012.59},
  abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\CYDYGKBJ\\Ji et al. - 2013 - 3D Convolutional Neural Networks for Human Action .pdf;C\:\\Users\\renan\\Zotero\\storage\\LH9LMTIB\\6165309.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {3D CNN model,3D convolution,3D convolutional neural networks,action recognition,airport surveillance videos,Algorithms,automated human action recognition,baseline methods,complex handcrafted features,Computational modeling,Computer architecture,convolutional neural networks,Decision Support Techniques,Deep learning,deep model,feature extraction,Feature extraction,feature representation,gesture recognition,high-level features,image classification,Image Interpretation; Computer-Assisted,image motion analysis,image representation,Imaging; Three-Dimensional,Kernel,model combination,motion information encoding,Movement,neural nets,Neural Networks (Computer),Pattern Recognition; Automated,Solid modeling,spatial dimensions,spatiotemporal phenomena,Subtraction Technique,temporal dimensions,Three dimensional displays,video surveillance,Videos},
  note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {1}
}

@inproceedings{karpathyLargeScaleVideoClassification2014,
  title = {Large-{{Scale Video Classification}} with {{Convolutional Neural Networks}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and {Fei-Fei}, Li},
  year = {2014},
  month = jun,
  pages = {1725--1732},
  publisher = {{IEEE}},
  address = {{Columbus, OH, USA}},
  doi = {10.1109/CVPR.2014.223},
  abstract = {Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on largescale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3\% to 63.9\%), but only a surprisingly modest improvement compared to single-frame models (59.3\% to 60.9\%). We further study the generalization performance of our best model by retraining the top layers on the UCF101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3\% up from 43.9\%).},
  file = {C\:\\Users\\renan\\Zotero\\storage\\GAFPKLB7\\Karpathy et al. - 2014 - Large-Scale Video Classification with Convolutiona.pdf},
  isbn = {978-1-4799-5118-5},
  language = {en}
}

@inproceedings{kazemiOneMillisecondFace2014,
  title = {One Millisecond Face Alignment with an Ensemble of Regression Trees},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Kazemi, Vahid and Sullivan, Josephine},
  year = {2014},
  month = jun,
  pages = {1867--1874},
  publisher = {{IEEE}},
  address = {{Columbus, OH}},
  doi = {10.1109/CVPR.2014.241},
  abstract = {This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face's landmark positions directly from a sparse subset of pixel intensities, achieving super-realtime performance with high quality predictions. We present a general framework based on gradient boosting for learning an ensemble of regression trees that optimizes the sum of square error loss and naturally handles missing or partially labelled data. We show how using appropriate priors exploiting the structure of image data helps with efficient feature selection. Different regularization strategies and its importance to combat overfitting are also investigated. In addition, we analyse the effect of the quantity of training data on the accuracy of the predictions and explore the effect of data augmentation using synthesized data.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\7CIFBZ8V\\Kazemi and Sullivan - 2014 - One millisecond face alignment with an ensemble of.pdf},
  isbn = {978-1-4799-5118-5},
  language = {en}
}

@inproceedings{Kluyver:2016aa,
  title = {Jupyter {{Notebooks}} \textendash{} a Publishing Format for Reproducible Computational Workflows},
  booktitle = {Positioning and Power in Academic Publishing: {{Players}}, Agents and Agendas},
  author = {Kluyver, Thomas and {Ragan-Kelley}, Benjamin and P{\'e}rez, Fernando and Granger, Brian and Bussonnier, Matthias and Frederic, Jonathan and Kelley, Kyle and Hamrick, Jessica and Grout, Jason and Corlay, Sylvain and Ivanov, Paul and Avila, Dami{\'a}n and Abdalla, Safia and Willing, Carol},
  editor = {Loizides, F. and Schmidt, B.},
  year = {2016},
  pages = {87--90},
  organization = {{IOS Press}}
}

@inproceedings{lelanSpeakerDiarizationUnsupervised2016,
  title = {Speaker {{Diarization With Unsupervised Training Framework}}},
  booktitle = {41st {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}} 2016)},
  author = {Le Lan, Ga{\"e}l and Meignier, Sylvain and Charlet, Delphine and Del{\'e}glise, Paul},
  year = {2016},
  month = mar,
  pages = {5},
  address = {{Shanghai, China}},
  doi = {10.1109/ICASSP.2016.7472741},
  abstract = {This paper investigates single and cross-show diarization based on an unsupervised i-vector framework, on French TV and Radio corpora. This framework uses speaker clustering as a way to automatically select data from unlabeled corpora to train i-vector PLDA models. Performances between supervised and unsupervised models are compared. The experimental results on two distinct test corpora (one TV, one Radio) show that unsupervised models perform as good as supervised models for both tasks. Such results indicate that performing an effective cross-show diarization on new language or new domain data in the future should not depend on the availability of manually annotated data.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\WDAI2TCA\\Le Lan et al. - 2016 - Speaker Diarization With Unsupervised Training Fra.pdf}
}

@inproceedings{mccowanAMIMeetingCorpus2005,
  title = {The {{AMI Meeting Corpus}}},
  booktitle = {In: {{Proceedings Measuring Behavior}} 2005, 5th {{International Conference}} on {{Methods}} and {{Techniques}} in {{Behavioral Research}}. {{L}}.{{P}}.{{J}}.{{J}}. {{Noldus}}, {{F}}. {{Grieco}}, {{L}}.{{W}}.{{S}}. {{Loijens}} and {{P}}.{{H}}. {{Zimmerman}} ({{Eds}}.), {{Wageningen}}: {{Noldus Information Technology}}},
  author = {Mccowan, I. and Lathoud, G. and Lincoln, M. and Lisowska, A. and Post, W. and Reidsma, D. and Wellner, P.},
  year = {2005},
  abstract = {To support multi-disciplinary research in the AMI (Augmented Multi-party Interaction) project, a 100 hour corpus of meetings is being collected. This corpus is being recorded in several instrumented rooms equipped with a variety of microphones, video cameras, electronic pens, presentation slide capture and white-board capture devices. As well as real meetings, the corpus contains a significant proportion of scenario-driven meetings, which have been designed to elicit a rich range of realistic behaviors. To facilitate research, the raw data are being annotated at a number of levels including speech transcriptions, dialogue acts and summaries. The corpus is being distributed using a web server designed to allow convenient browsing and download of multimedia content and associated annotations. This article first overviews AMI research themes, then discusses corpus design, as well as data collection, annotation and distribution.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\W3JCAPJK\\Mccowan et al. - 2005 - The AMI Meeting Corpus.pdf;C\:\\Users\\renan\\Zotero\\storage\\FHP28W4E\\summary.html}
}

@article{mccullochLogicalCalculusIdeas1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S. and Pitts, Walter},
  year = {1943},
  month = dec,
  volume = {5},
  pages = {115--133},
  issn = {1522-9602},
  doi = {10.1007/BF02478259},
  abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\TQZHHSCE\\McCulloch and Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf},
  journal = {The bulletin of mathematical biophysics},
  language = {en},
  number = {4}
}

@inproceedings{mckinney-proc-scipy-2010,
  title = {Data {{Structures}} for {{Statistical Computing}} in {{Python}}},
  booktitle = {Proceedings of the 9th {{Python}} in {{Science Conference}}},
  author = {{Wes McKinney}},
  editor = {{van der Walt}, St{\'e}fan and {Jarrod Millman}},
  year = {2010},
  pages = {51--56}
}

@misc{mtheilerDeteccaoMarcadoresFaciais2019,
  title = {Detec{\c c}{\~a}o de {{Marcadores Faciais}} Pela Biblioteca {{Dlib}}},
  shorttitle = {English},
  author = {{MTheiler}},
  year = {2019},
  month = jan,
  url = {https://commons.wikimedia.org/wiki/File:Dlib-face_landmark_detection.jpg},
  urldate = {2020-02-25},
  file = {C\:\\Users\\renan\\Zotero\\storage\\M34H965E\\FileDlib-face_landmark_detection.html}
}

@article{opencv_library,
  title = {The {{OpenCV}} Library},
  author = {Bradski, G.},
  year = {2000},
  citeulike-article-id = {2236121},
  journal = {Dr. Dobb's Journal of Software Tools},
  keywords = {bibtex-import},
  posted-at = {2008-01-15 19:21:54},
  priority = {4}
}

@article{oxenhamPitchPerception2012,
  title = {Pitch {{Perception}}},
  author = {Oxenham, Andrew J.},
  year = {2012},
  month = sep,
  volume = {32},
  pages = {13335--13338},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3815-12.2012},
  abstract = {Pitch is one of the primary auditory sensations and plays a defining role in music, speech, and auditory scene analysis. Although the main physical correlate of pitch is acoustic periodicity, or repetition rate, there are many interactions that complicate the relationship between the physical stimulus and the perception of pitch. In particular, the effects of other acoustic parameters on pitch judgments, and the complex interactions between perceptual organization and pitch, have uncovered interesting perceptual phenomena that should help to reveal the underlying neural mechanisms.},
  copyright = {Copyright \textcopyright{} 2012 the authors 0270-6474/12/3213335-04\$15.00/0},
  file = {C\:\\Users\\renan\\Zotero\\storage\\4UP3WFTR\\Oxenham - 2012 - Pitch Perception.pdf;C\:\\Users\\renan\\Zotero\\storage\\GIDP5F5Y\\13335.html},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {39},
  pmid = {23015422}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The {{Perceptron}}: {{A Probabilistic Model}} for {{Information Storage}} and {{Organization}} in {{The Brain}}},
  shorttitle = {The {{Perceptron}}},
  author = {Rosenblatt, F.},
  year = {1958},
  pages = {65--386},
  abstract = {If we are eventually to understand the capability of higher organisms for perceptual recognition, generalization, recall, and thinking, we must first have answers to three fundamental questions: 1. How is information about the physical world sensed, or detected, by the biological system? 2. In what form is information stored, or remembered? 3. How does information contained in storage, or in memory, influence recognition and behavior? The first of these questions is in the},
  file = {C\:\\Users\\renan\\Zotero\\storage\\85I9GKY8\\Rosenblatt - 1958 - The Perceptron A Probabilistic Model for Informat.pdf;C\:\\Users\\renan\\Zotero\\storage\\RHY68RX7\\summary.html},
  journal = {Psychological Review}
}

@inproceedings{sellSpeakerDiarizationPlda2014,
  title = {Speaker Diarization with Plda I-Vector Scoring and Unsupervised Calibration},
  booktitle = {2014 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Sell, Gregory and {Garcia-Romero}, Daniel},
  year = {2014},
  month = dec,
  pages = {413--417},
  issn = {null},
  doi = {10.1109/SLT.2014.7078610},
  abstract = {Speaker diarization via unsupervised i-vector clustering has gained popularity in recent years. In this approach, i-vectors are extracted from short clips of speech segmented from a larger multi-speaker conversation and organized into speaker clusters, typically according to their cosine score. In this paper, we propose a system that incorporates probabilistic linear discriminant analysis (PLDA) for i-vector scoring, a method already frequently utilized in speaker recognition tasks, and uses unsupervised calibration of the PLDA scores to determine the clustering stopping criterion. We also demonstrate that denser sampling in the i-vector space with overlapping temporal segments provides a gain in the diarization task. We test our system on the CALLHOME conversational telephone speech corpus, which includes multiple languages and a varying number of speakers, and we show that PLDA scoring outperforms the same system with cosine scoring, and that overlapping segments reduce diarization error rate (DER) as well.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\PMETV7U2\\Sell and Garcia-Romero - 2014 - Speaker diarization with plda i-vector scoring and.pdf;C\:\\Users\\renan\\Zotero\\storage\\6Q9TFBAT\\7078610.html},
  keywords = {calibration,Calibration,CALLHOME conversational telephone speech corpus,cosine score,denser sampling,Density estimation robust algorithm,DER,diarization error rate,multispeaker conversation,pattern clustering,PLDA i-vector scoring,Principal component analysis,probabilistic linear discriminant analysis,sampling methods,segmented speech,speaker clusters,speaker diarization,speaker recognition,Speaker recognition,speaker recognition tasks,Speech,Speech processing,unsupervised calibration,unsupervised i-vector clustering}
}

@inproceedings{snyderXVectorsRobustDNN2018,
  title = {X-{{Vectors}}: {{Robust DNN Embeddings}} for {{Speaker Recognition}}},
  shorttitle = {X-{{Vectors}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Snyder, David and {Garcia-Romero}, Daniel and Sell, Gregory and Povey, Daniel and Khudanpur, Sanjeev},
  year = {2018},
  month = apr,
  pages = {5329--5333},
  publisher = {{IEEE}},
  address = {{Calgary, AB}},
  doi = {10.1109/ICASSP.2018.8461375},
  abstract = {In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\95UVGLQV\\Snyder et al. - 2018 - X-Vectors Robust DNN Embeddings for Speaker Recog.pdf},
  isbn = {978-1-5386-4658-8},
  language = {en}
}

@article{tensorflow2015-whitepaper,
  title = {{{TensorFlow}}: {{Large}}-Scale {{Machine Learning}} on {{Heterogeneous Systems}}},
  author = {Abadi, Mart{\'i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'e}, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'e}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2015},
  url = {https://www.tensorflow.org/},
  abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, generalpurpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous ``parameter server'' designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.}
}

@inproceedings{wangSpeakerDiarizationLSTM2018,
  title = {Speaker {{Diarization}} with {{LSTM}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wang, Quan and Downey, Carlton and Wan, Li and Mansfield, Philip Andrew and Moreno, Ignacio Lopz},
  year = {2018},
  month = apr,
  pages = {5239--5243},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2018.8462628},
  abstract = {For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and speaker diarization applications. However, mirroring the rise of deep learning in various domains, neural network based audio embeddings, also known as d-vectors, have consistently demonstrated superior speaker verification performance. In this paper, we build on the success of d-vector based speaker verification systems to develop a new d-vector based approach to speaker diarization. Specifically, we combine LSTM-based d-vector audio embeddings with recent work in non-parametric clustering to obtain a state-of-the-art speaker diarization system. Our system is evaluated on three standard public datasets, suggesting that d-vector based diarization systems offer significant advantages over traditional i-vector based systems. We achieved a 12.0\% diarization error rate on NIST SRE 2000 CALLHOME, while our model is trained with out-of-domain data from voice search logs.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\YSEJMTVN\\Wang et al. - 2018 - Speaker Diarization with LSTM.pdf;C\:\\Users\\renan\\Zotero\\storage\\7QZAM5ZR\\8462628.html},
  keywords = {audio embedding,audio signal processing,Clustering algorithms,d-vector based diarization systems,d-vector based speaker verification systems,deep learning,diarization error rate,Feature extraction,i-vector based audio embedding techniques,i-vector based systems,LSTM,LSTM-based d-vector audio embeddings,Machine learning,Neural networks,NIST,recurrent neural nets,Speaker diarization,speaker diarization system,speaker recognition,spectral clustering,Voice activity detection}
}

@book{werbosRegressionNewTools1975,
  title = {Beyond {{Regression}}: {{New Tools}} for {{Prediction}} and {{Analysis}} in the {{Behavioral Sciences}}},
  shorttitle = {Beyond {{Regression}}},
  author = {Werbos, Paul John},
  year = {1975},
  publisher = {{Harvard University}},
  googlebooks = {z81XmgEACAAJ},
  language = {en}
}

@article{zewoudieUseLongtermFeatures2018,
  title = {The Use of Long-Term Features for {{GMM}}- and i-Vector-Based Speaker Diarization Systems},
  author = {Zewoudie, Abraham Woubie and Luque, Jordi and Hernando, Javier},
  year = {2018},
  month = sep,
  volume = {2018},
  pages = {14},
  issn = {1687-4722},
  doi = {10.1186/s13636-018-0140-x},
  abstract = {Several factors contribute to the performance of speaker diarization systems. For instance, the appropriate selection of speech features is one of the key aspects that affect speaker diarization systems. The other factors include the techniques employed to perform both segmentation and clustering. While the static mel frequency cepstral coefficients are the most widely used features in speech-related tasks including speaker diarization, several studies have shown the benefits of augmenting regular speech features with the static ones.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\GXP27EB6\\Zewoudie et al. - 2018 - The use of long-term features for GMM- and i-vecto.pdf},
  journal = {EURASIP Journal on Audio, Speech, and Music Processing},
  language = {en},
  number = {1}
}


