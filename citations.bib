
@article{abiodunStateoftheartArtificialNeural2018,
  title = {State-of-the-Art in Artificial Neural Network Applications: {{A}} Survey},
  shorttitle = {State-of-the-Art in Artificial Neural Network Applications},
  author = {Abiodun, Oludare Isaac and Jantan, Aman and Omolara, Abiodun Esther and Dada, Kemi Victoria and Mohamed, Nachaat AbdElatif and Arshad, Humaira},
  year = {2018},
  month = nov,
  volume = {4},
  pages = {e00938},
  issn = {2405-8440},
  doi = {10.1016/j.heliyon.2018.e00938},
  abstract = {This is a survey of neural network applications in the real-world scenario. It provides a taxonomy of artificial neural networks (ANNs) and furnish the reader with knowledge of current and emerging trends in ANN applications research and area of focus for researchers. Additionally, the study presents ANN application challenges, contributions, compare performances and critiques methods. The study covers many applications of ANN techniques in various disciplines which include computing, science, engineering, medicine, environmental, agriculture, mining, technology, climate, business, arts, and nanotechnology, etc. The study assesses ANN contributions, compare performances and critiques methods. The study found that neural-network models such as feedforward and feedback propagation artificial neural networks are performing better in its application to human problems. Therefore, we proposed feedforward and feedback propagation ANN models for research focus based on data analysis factors like accuracy, processing speed, latency, fault tolerance, volume, scalability, convergence, and performance. Moreover, we recommend that instead of applying a single method, future research can focus on combining ANN models into one network-wide application.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\U5GSS872\\Abiodun et al. - 2018 - State-of-the-art in artificial neural network appl.pdf;C\:\\Users\\renan\\Zotero\\storage\\DZI6SS7A\\S2405844018332067.html},
  journal = {Heliyon},
  keywords = {Computer science},
  language = {en},
  number = {11}
}

@article{akramSequenceSequenceWeather2016,
  title = {Sequence to {{Sequence Weather Forecasting}} with {{Long Short}}-{{Term Memory Recurrent Neural Networks}}},
  author = {Akram, Mohamed and El, Chaker},
  year = {2016},
  month = jun,
  volume = {143},
  pages = {7--11},
  issn = {09758887},
  doi = {10.5120/ijca2016910497},
  abstract = {The aim of this paper is to present a deep neural network architecture and use it in time series weather prediction. It uses multi stacked LSTMs to map sequences of weather values of the same length. The final goal is to produce two types of models per city (for 9 cities in Morocco) to forecast 24 and 72 hours worth of weather data (for Temperature, Humidity and Wind Speed). Approximately 15 years (2000-2015) of hourly meteorological data was used to train the model. The results show that LSTM based neural networks are competitive with the traditional methods and can be considered a better alternative to forecast general weather conditions.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\JPVBCDSA\\Akram and El - 2016 - Sequence to Sequence Weather Forecasting with Long.pdf},
  journal = {International Journal of Computer Applications},
  language = {en},
  number = {11}
}

@article{barbosaSyllabletimingBrazilianPortuguese2000,
  title = {"{{Syllable}}-Timing in {{Brazilian Portuguese}}": Uma Cr{\'i}tica a {{Roy Major}}},
  shorttitle = {"{{Syllable}}-Timing in {{Brazilian Portuguese}}"},
  author = {Barbosa, Pl{\'i}nio Almeida},
  year = {2000},
  volume = {16},
  pages = {369--402},
  issn = {0102-4450},
  doi = {10.1590/S0102-44502000000200006},
  abstract = {A tese de R. Major, segundo a qual haveria evid{\^e}ncias para se considerar o portugu{\^e}s brasileiro (PB) como "stress-timing" ou tendendo para tal, {\'e} rediscutida. As quest{\~o}es fon{\'e}tico-fonol{\'o}gicas suscitadas pela dicotomia de l{\'i}nguas "stress-timed" e "syllable-timed" e o suposto isocronismo absoluto s{\~a}o apresentadas sob um prisma estritamente pros{\'o}dico-temporal. Um modelo empregando dois osciladores acoplados (acentual e sil{\'a}bico) possibilita a caracteriza{\c c}{\~a}o biparam{\'e}trica (taxa de elocu{\c c}{\~a}o e for{\c c}a de acoplamento) de um conjunto arbitr{\'a}rio de frases de uma l{\'i}ngua e permite mostrar que, em PB, h{\'a} alto grau de "syllable-timing". {\`A} luz de uma an{\'a}lise fon{\'e}tica mais cuidadosa dos fatores ligados ao ritmo, mostra-se que os argumentos apresentados por Major para justificar "stress-timing" em PB s{\~a}o completamente equivocados.
          , 
            This paper reintroduces the discussion about stress-timing in Brazilian Portuguese (BP). It begins by surveying some phonetic and phonological issues raised by the syllable- vs stress-timed dichotomy which culminated with the emergence of the p-center notion. Strict considerations of timing of V-V units and stress groups are taken into account to analyze the long term coupling of two basic oscillators (vowel and stress flow). This coupling allows a two-parameter characterization of language rhythms (coupling strength and speech rate) revealing that BP utterances present a high-degree of syllable-timing. A comparison with other languages, including European Portuguese, is also presented. The results analyzed indicate that Major's arguments for considering Portuguese (sic) as stress-timing are misleading.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\95Y6SX5G\\Barbosa - 2000 - Syllable-timing in Brazilian Portuguese uma cr√≠.pdf},
  journal = {DELTA: Documenta{\c c}{\~a}o de Estudos em Ling{\"u}{\'i}stica Te{\'o}rica e Aplicada},
  number = {2}
}

@book{behnkeHierarchicalNeuralNetworks2003,
  title = {Hierarchical {{Neural Networks}} for {{Image Interpretation}}},
  author = {Behnke, Sven},
  year = {2003},
  month = jan,
  volume = {2766},
  doi = {10.1007/b11963},
  abstract = {I. Theory.- Neurobiological Background.- Related Work.- Neural Abstraction Pyramid Architecture.- Unsupervised Learning.- Supervised Learning.- II. Applications.- Recognition of Meter Values.- Binarization of Matrix Codes.- Learning Iterative Image Reconstruction.- Face Localization.- Summary and Conclusions.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\XTYJURS5\\Behnke - 2003 - Hierarchical Neural Networks for Image Interpretat.pdf},
  isbn = {978-3-540-40722-5},
  note = {Journal Abbreviation: Lecture Notes in Computer Science
Publication Title: Lecture Notes in Computer Science}
}

@article{bojarskiEndEndLearning2016,
  title = {End to {{End Learning}} for {{Self}}-{{Driving Cars}}},
  author = {Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D. and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and Zhang, Xin and Zhao, Jake and Zieba, Karol},
  year = {2016},
  month = apr,
  url = {http://arxiv.org/abs/1604.07316},
  urldate = {2020-03-06},
  abstract = {We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).},
  archivePrefix = {arXiv},
  eprint = {1604.07316},
  eprinttype = {arxiv},
  file = {C\:\\Users\\renan\\Zotero\\storage\\UZERDYL5\\Bojarski et al. - 2016 - End to End Learning for Self-Driving Cars.pdf;C\:\\Users\\renan\\Zotero\\storage\\PQTRBE8N\\1604.html},
  journal = {arXiv:1604.07316 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@article{caoFaceAlignmentExplicit2014,
  title = {Face {{Alignment}} by {{Explicit Shape Regression}}},
  author = {Cao, Xudong and Wei, Yichen and Wen, Fang and Sun, Jian},
  year = {2014},
  month = apr,
  volume = {107},
  pages = {177--190},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-013-0667-3},
  abstract = {We present a very efficient, highly accurate, ``Explicit Shape Regression'' approach for face alignment. Unlike previous regression-based approaches, we directly learn a vectorial regression function to infer the whole facial shape (a set of facial landmarks) from the image and explicitly minimize the alignment errors over the training data. The inherent shape constraint is naturally encoded into the regressor in a cascaded learning framework and applied from coarse to fine during the test, without using a fixed parametric shape model as in most previous methods. To make the regression more effective and efficient, we design a two-level boosted regression, shape indexed features and a correlation-based feature selection method. This combination enables us to learn accurate models from large training data in a short time (20 min for 2,000 training images), and run regression extremely fast in test (15 ms for a 87 landmarks shape). Experiments on challenging data show that our approach significantly outperforms the state-of-the-art in terms of both accuracy and efficiency.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\EE54IZAB\\Cao et al. - 2014 - Face Alignment by Explicit Shape Regression.pdf},
  journal = {International Journal of Computer Vision},
  language = {en},
  number = {2}
}

@inproceedings{dalalHistogramsOrientedGradients2005,
  title = {Histograms of {{Oriented Gradients}} for {{Human Detection}}},
  booktitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  author = {Dalal, N. and Triggs, B.},
  year = {2005},
  volume = {1},
  pages = {886--893},
  publisher = {{IEEE}},
  address = {{San Diego, CA, USA}},
  doi = {10.1109/CVPR.2005.177},
  abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\7I8IHMMA\\Dalal and Triggs - 2005 - Histograms of Oriented Gradients for Human Detecti.pdf},
  isbn = {978-0-7695-2372-9},
  language = {en}
}

@inproceedings{dantoneRealtimeFacialFeature2012,
  title = {Real-Time Facial Feature Detection Using Conditional Regression Forests},
  booktitle = {2012 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Dantone, Matthias and Gall, Juergen and Fanelli, Gabriele and Van Gool, Luc},
  year = {2012},
  month = jun,
  pages = {2578--2585},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2012.6247976},
  abstract = {Although facial feature detection from 2D images is a well-studied field, there is a lack of real-time methods that estimate feature points even on low quality images. Here we propose conditional regression forest for this task. While regression forest learn the relations between facial image patches and the location of feature points from the entire set of faces, conditional regression forest learn the relations conditional to global face properties. In our experiments, we use the head pose as a global property and demonstrate that conditional regression forests outperform regression forests for facial feature detection. We have evaluated the method on the challenging Labeled Faces in the Wild [20] database where close-to-human accuracy is achieved while processing images in real-time.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\SSEZRXET\\6247976.html},
  keywords = {2D images,Accuracy,conditional regression forests,Databases,Facial features,facial image,feature extraction,global face properties,Head,head pose,Real time systems,real-time facial feature detection,regression analysis,Training,trees (mathematics),Vegetation}
}

@article{dehakFrontEndFactorAnalysis2011,
  title = {Front-{{End Factor Analysis}} for {{Speaker Verification}}},
  author = {Dehak, Najim and Kenny, Patrick J. and Dehak, R{\'e}da and Dumouchel, Pierre and Ouellet, Pierre},
  year = {2011},
  month = may,
  volume = {19},
  pages = {788--798},
  issn = {1558-7924},
  doi = {10.1109/TASL.2010.2064307},
  abstract = {This paper presents an extension of our previous work which proposes a new speaker representation for speaker verification. In this modeling, a new low-dimensional speaker- and channel-dependent space is defined using a simple factor analysis. This space is named the total variability space because it models both speaker and channel variabilities. Two speaker verification systems are proposed which use this new representation. The first system is a support vector machine-based system that uses the cosine kernel to estimate the similarity between the input data. The second system directly uses the cosine similarity as the final decision score. We tested three channel compensation techniques in the total variability space, which are within-class covariance normalization (WCCN), linear discriminate analysis (LDA), and nuisance attribute projection (NAP). We found that the best results are obtained when LDA is followed by WCCN. We achieved an equal error rate (EER) of 1.12\% and MinDCF of 0.0094 using the cosine distance scoring on the male English trials of the core condition of the NIST 2008 Speaker Recognition Evaluation dataset. We also obtained 4\% absolute EER improvement for both-gender trials on the 10 s-10 s condition compared to the classical joint factor analysis scoring.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\C52WASEX\\Dehak et al. - 2011 - Front-End Factor Analysis for Speaker Verification.pdf;C\:\\Users\\renan\\Zotero\\storage\\2WDMW3E4\\5545402.html},
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  keywords = {channel compensation technique,channel dependent space,Context modeling,Cosine distance scoring,cosine kernel,decision score,front end factor analysis,joint factor analysis (JFA),Kernel,Linear discriminant analysis,linear discriminate analysis,low dimensional speaker,Natural languages,NIST,nuisance attribute projection,Permission,similarity estimation,speaker recognition,Speaker recognition,speaker recognition evaluation,speaker representation,speaker verification,Speech analysis,support vector machine,support vector machines,Support vector machines,support vector machines (SVMs),Testing,total variability space,variability space,within class covariance normalization},
  note = {Conference Name: IEEE Transactions on Audio, Speech, and Language Processing},
  number = {4}
}

@misc{dhp1080IdoSkemoPri2016,
  title = {Ido: {{Skemo}} Pri La Kompozanti Di Un Neurono.},
  shorttitle = {Ido},
  author = {{Dhp1080}},
  year = {2016},
  month = oct,
  url = {https://commons.wikimedia.org/wiki/File:Neurono-ido.svg\#/media/File:Neurono-ido.svg},
  urldate = {2020-03-06},
  copyright = {Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.2 or any later version published by the Free Software Foundation; with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A copy of the license is included in the section entitled GNU Free Documentation License.http://www.gnu.org/copyleft/fdl.htmlGFDLGNU Free Documentation Licensetruetrue},
  file = {C\:\\Users\\renan\\Zotero\\storage\\T698JZVB\\FileNeurono-ido.html}
}

@misc{diliffEnglishLookingEast2015,
  title = {English: {{Looking}} East from the Rear of the Nave of {{St Augustine}}'s {{Church}} in {{Kilburn}}, a Northern Suburb of {{London}}, {{England}}.},
  shorttitle = {English},
  author = {{Diliff}},
  year = {2015},
  month = feb,
  url = {https://commons.wikimedia.org/wiki/File:St_Augustine\%27s_Church,_Kilburn_Interior_1,_London,_UK_-_Diliff.jpg},
  urldate = {2020-03-10},
  copyright = {Permission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation License, Version 1.2 or any later version published by the Free Software Foundation; with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts. A copy of the license is included in the section entitled GNU Free Documentation License.http://www.gnu.org/copyleft/fdl.htmlGFDLGNU Free Documentation Licensetruetrue},
  file = {C\:\\Users\\renan\\Zotero\\storage\\VCVDKCQF\\FileSt_Augustine's_Church,_Kilburn_Interior_1,_London,_UK_-_Diliff.html}
}

@article{dimitriadisEnhancementsAudioonlyDiarization2019,
  title = {Enhancements for {{Audio}}-Only {{Diarization Systems}}},
  author = {Dimitriadis, Dimitrios},
  year = {2019},
  month = aug,
  url = {http://arxiv.org/abs/1909.00082},
  urldate = {2020-02-08},
  abstract = {In this paper two different approaches to enhance the performance of the most challenging component of a Speaker Diarization system are presented, i.e. the speaker clustering part. A processing step is proposed enhancing the input features with a temporal smoothing process combined with nonlinear filtering. We, also, propose improvements on the Deep Embedded Clustering (DEC) algorithm -- a nonlinear feature transformation. The performance of these enhancements is compared with different clustering algorithms, such as the UISRNN, k-Means, Spectral clustering and x-Means. The evaluation is held on three different tasks, i.e. the AMI, DIHARD and an internal meeting transcription task. The proposed approaches assume a known number of speakers and time segmentations for the audio files. Since, we focus only on the clustering component of diarization for this work, the segmentation provided is assumed perfect. Finally, we present how supervision, in the form of given speaker profiles, can further improve the overall diarization performance. The proposed enhancements yield substantial relative improvements in all 3 tasks, with 20\textbackslash\% in AMI and 19\textbackslash\% better than the best diarization system for DIHARD task, when the number of speakers is known.},
  archivePrefix = {arXiv},
  eprint = {1909.00082},
  eprinttype = {arxiv},
  file = {C\:\\Users\\renan\\Zotero\\storage\\8P4SLIKC\\Dimitriadis - 2019 - Enhancements for Audio-only Diarization Systems.pdf;C\:\\Users\\renan\\Zotero\\storage\\WUVWNVCG\\1909.html},
  journal = {arXiv:1909.00082 [cs, eess]},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{dlib09,
  title = {Dlib-Ml: {{A}} Machine Learning Toolkit},
  author = {King, Davis E.},
  year = {2009},
  volume = {10},
  pages = {1755--1758},
  journal = {Journal of Machine Learning Research}
}

@article{dreyfusArtificialNeuralNetworks1990,
  title = {Artificial Neural Networks, Back Propagation, and the {{Kelley}}-{{Bryson}} Gradient Procedure},
  author = {Dreyfus, Stuart E.},
  year = {1990},
  month = sep,
  volume = {13},
  pages = {926--928},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/3.25422},
  file = {C\:\\Users\\renan\\Zotero\\storage\\N9PQCHE2\\3.html},
  journal = {Journal of Guidance, Control, and Dynamics},
  number = {5}
}

@article{ephratLookingListenCocktail2018,
  title = {Looking to {{Listen}} at the {{Cocktail Party}}: {{A Speaker}}-{{Independent Audio}}-{{Visual Model}} for {{Speech Separation}}},
  shorttitle = {Looking to {{Listen}} at the {{Cocktail Party}}},
  author = {Ephrat, Ariel and Mosseri, Inbar and Lang, Oran and Dekel, Tali and Wilson, Kevin and Hassidim, Avinatan and Freeman, William T. and Rubinstein, Michael},
  year = {2018},
  month = jul,
  volume = {37},
  pages = {1--11},
  issn = {07300301},
  doi = {10.1145/3197517.3201357},
  abstract = {We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to "focus" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVSpeech, a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).},
  archivePrefix = {arXiv},
  eprint = {1804.03619},
  eprinttype = {arxiv},
  file = {C\:\\Users\\renan\\Zotero\\storage\\658VDDKT\\Ephrat et al. - 2018 - Looking to Listen at the Cocktail Party A Speaker.pdf;C\:\\Users\\renan\\Zotero\\storage\\9WEKHLPB\\1804.html},
  journal = {ACM Transactions on Graphics},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  number = {4}
}

@article{felzenszwalbObjectDetectionDiscriminatively,
  title = {Object {{Detection}} with {{Discriminatively Trained Part Based Models}}},
  author = {Felzenszwalb, Pedro F and Girshick, Ross B and McAllester, David and Ramanan, Deva},
  pages = {20},
  abstract = {We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL datasets. Our system relies on new methods for discriminative training with partially labeled data. We combine a marginsensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI-SVM in terms of latent variables. A latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\43XBKYPZ\\Felzenszwalb et al. - Object Detection with Discriminatively Trained Par.pdf},
  language = {en}
}

@article{freemanOrientationHistogramsHand,
  title = {Orientation {{Histograms}} for {{Hand Gesture Recognition}}},
  author = {Freeman, William T and Roth, Michal},
  pages = {9},
  abstract = {We present a method to recognize hand gestures, based on a pattern recognition technique developed by McConnell [16] employing histograms of local orientation. We use the orientation histogram as a feature vector for gesture classification and interpolation. For moving or d\textasciidieresis{}ynamic gestures\textasciidieresis, the histogram of the spatio-temporal gradients of image intensity form the analogous feature vector and may be useful for dynamic gesture recognition.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\VTH44ILA\\Freeman and Roth - Orientation Histograms for Hand Gesture Recognitio.pdf},
  language = {en}
}

@article{hannunCardiologistLevelArrhythmiaDetection2019,
  title = {Cardiologist-{{Level Arrhythmia Detection}} and {{Classification}} in {{Ambulatory Electrocardiograms Using}} a {{Deep Neural Network}}},
  author = {Hannun, Awni Y. and Rajpurkar, Pranav and Haghpanahi, Masoumeh and Tison, Geoffrey H. and Bourn, Codie and Turakhia, Mintu P. and Ng, Andrew Y.},
  year = {2019},
  month = jan,
  volume = {25},
  pages = {65--69},
  issn = {1078-8956},
  doi = {10.1038/s41591-018-0268-3},
  abstract = {Computerized electrocardiogram (ECG) interpretation plays a critical role in the clinical ECG workflow. Widely available digital ECG data and the algorithmic paradigm of deep learning present an opportunity to substantially improve the accuracy and scalability of automated ECG analysis. However, a comprehensive evaluation of an end-to-end deep learning approach for ECG analysis across a wide variety of diagnostic classes has not been previously reported. Here, we develop a deep neural network (DNN) to classify 12 rhythm classes using 91,232 single-lead ECGs from 53,877 patients who used a single-lead ambulatory ECG monitoring device. When validated against an independent test dataset annotated by a consensus committee of board-certified practicing cardiologists, the DNN achieved an average area under the receiver operating characteristic curve (AUC) of 0.97. The average F1 score, which is the harmonic mean of the positive predictive value and sensitivity, for the DNN (0.837) exceeded that of average cardiologists (0.780). With specificity fixed at the average specificity achieved by cardiologists, the sensitivity of the DNN exceeded the average cardiologist sensitivity for all rhythm classes. These findings demonstrate that an end-to-end deep learning approach can classify a broad range of distinct arrhythmias from single-lead ECGs with high diagnostic performance similar to that of cardiologists. If confirmed in clinical settings, this approach could reduce the rate of misdiagnosed computerized ECG interpretations and improve the efficiency of expert-human ECG interpretation by accurately triaging or prioritizing the most urgent conditions.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\BBKNXSR6\\Hannun et al. - 2019 - Cardiologist-Level Arrhythmia Detection and Classi.pdf},
  journal = {Nature medicine},
  number = {1},
  pmcid = {PMC6784839},
  pmid = {30617320}
}

@inproceedings{hersheyAudiovisualGraphicalModels2004,
  title = {Audio-Visual Graphical Models for Speech Processing},
  booktitle = {Acoustics, {{Speech}}, and {{Signal Processing}}, 1988. {{ICASSP}}-88., 1988 {{International Conference}} On},
  author = {Hershey, John and Attias, H. and Jojic, Nebojsa and Kristjansson, Trausti},
  year = {2004},
  month = jun,
  volume = {5},
  pages = {V-649},
  doi = {10.1109/ICASSP.2004.1327194},
  abstract = {Perceiving sounds in a noisy environment is a challenging problem. Visual lip-reading can provide relevant information but is also challenging because lips are moving and a tracker must deal with a variety of conditions. Typically audio-visual systems have been assembled from individually engineered modules. We propose to fuse audio and video in a probabilistic generative model that implements cross-model self-supervised learning, enabling adaptation to audio-visual data. The video model features a Gaussian mixture model embedded in a linear subspace of a sprite which translates in the video. The system can learn to detect and enhance speech in noise given only a short (30 second) sequence of audio-visual data. We show some results for speech detection and enhancement, and discuss extensions to the model that are under investigation.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\NB5IPHQA\\Hershey et al. - 2004 - Audio-visual graphical models for speech processin.pdf}
}

@article{hintonFastLearningAlgorithm2006,
  title = {A {{Fast Learning Algorithm}} for {{Deep Belief Nets}}},
  author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
  year = {2006},
  month = jul,
  volume = {18},
  pages = {1527--1554},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2006.18.7.1527},
  file = {C\:\\Users\\renan\\Zotero\\storage\\RFBGEQJN\\Hinton et al. - 2006 - A Fast Learning Algorithm for Deep Belief Nets.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {7}
}

@article{houDeepSFDeepConvolutional2018,
  title = {{{DeepSF}}: Deep Convolutional Neural Network for Mapping Protein Sequences to Folds},
  shorttitle = {{{DeepSF}}},
  author = {Hou, Jie and Adhikari, Badri and Cheng, Jianlin},
  year = {2018},
  month = apr,
  volume = {34},
  pages = {1295--1303},
  publisher = {{Oxford Academic}},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btx780},
  abstract = {AbstractMotivation.  Protein fold recognition is an important problem in structural bioinformatics. Almost all traditional fold recognition methods use sequence},
  file = {C\:\\Users\\renan\\Zotero\\storage\\BYLAXHCN\\Hou et al. - 2018 - DeepSF deep convolutional neural network for mapp.pdf;C\:\\Users\\renan\\Zotero\\storage\\EKBLGCCG\\4708302.html},
  journal = {Bioinformatics},
  language = {en},
  number = {8}
}

@article{hunterMatplotlib2DGraphics2007,
  title = {Matplotlib: {{A 2D Graphics Environment}}},
  shorttitle = {Matplotlib},
  author = {Hunter, John D.},
  year = {2007},
  month = may,
  volume = {9},
  pages = {90--95},
  issn = {1558-366X},
  doi = {10.1109/MCSE.2007.55},
  abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting,and publication-quality image generation across user interfaces and operating systems},
  file = {C\:\\Users\\renan\\Zotero\\storage\\5Q4ITRYD\\Hunter - 2007 - Matplotlib A 2D Graphics Environment.pdf;C\:\\Users\\renan\\Zotero\\storage\\3P3S2ZSI\\4160265.html},
  journal = {Computing in Science Engineering},
  keywords = {2D graphics package,application development,computer graphics,Computer languages,Equations,Graphical user interfaces,Graphics,Image generation,interactive scripting,Interpolation,mathematics computing,Matplotlib,object-oriented programming,operating system,Operating systems,Packaging,Programming profession,publication-quality image generation,Python,scientific programming,scripting languages,software packages,user interface,User interfaces},
  note = {Conference Name: Computing in Science Engineering},
  number = {3}
}

@article{ji3DConvolutionalNeural2013,
  title = {{{3D Convolutional Neural Networks}} for {{Human Action Recognition}}},
  author = {Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
  year = {2013},
  month = jan,
  volume = {35},
  pages = {221--231},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2012.59},
  abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\CYDYGKBJ\\Ji et al. - 2013 - 3D Convolutional Neural Networks for Human Action .pdf;C\:\\Users\\renan\\Zotero\\storage\\LH9LMTIB\\6165309.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {3D CNN model,3D convolution,3D convolutional neural networks,action recognition,airport surveillance videos,Algorithms,automated human action recognition,baseline methods,complex handcrafted features,Computational modeling,Computer architecture,convolutional neural networks,Decision Support Techniques,Deep learning,deep model,feature extraction,Feature extraction,feature representation,gesture recognition,high-level features,image classification,Image Interpretation; Computer-Assisted,image motion analysis,image representation,Imaging; Three-Dimensional,Kernel,model combination,motion information encoding,Movement,neural nets,Neural Networks (Computer),Pattern Recognition; Automated,Solid modeling,spatial dimensions,spatiotemporal phenomena,Subtraction Technique,temporal dimensions,Three dimensional displays,video surveillance,Videos},
  note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {1}
}

@inproceedings{karpathyLargeScaleVideoClassification2014,
  title = {Large-{{Scale Video Classification}} with {{Convolutional Neural Networks}}},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and {Fei-Fei}, Li},
  year = {2014},
  month = jun,
  pages = {1725--1732},
  publisher = {{IEEE}},
  address = {{Columbus, OH, USA}},
  doi = {10.1109/CVPR.2014.223},
  abstract = {Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on largescale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3\% to 63.9\%), but only a surprisingly modest improvement compared to single-frame models (59.3\% to 60.9\%). We further study the generalization performance of our best model by retraining the top layers on the UCF101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3\% up from 43.9\%).},
  file = {C\:\\Users\\renan\\Zotero\\storage\\GAFPKLB7\\Karpathy et al. - 2014 - Large-Scale Video Classification with Convolutiona.pdf},
  isbn = {978-1-4799-5118-5},
  language = {en}
}

@inproceedings{kazemiFaceAlignmentPartBased2011,
  title = {Face {{Alignment}} with {{Part}}-{{Based Modeling}}},
  booktitle = {{{BMVC}} 2011 - {{Proceedings}} of the {{British Machine Vision Conference}} 2011},
  author = {Kazemi, Vahid and Sullivan, Josephine},
  year = {2011},
  month = sep,
  pages = {27.1-27.10},
  doi = {10.5244/C.25.27},
  abstract = {We propose a new method for face alignment with part-based modeling. This method is competitive in terms of precision with existing methods such as Active Appearance Models, but is more robust and has a superior generalization ability due to its part-based nature. A variation of the Histogram of Oriented Gradients descriptor is used to model the appearance of each part and the shape information is represented with a set of landmark points around the major facial features. Multiple linear regression models are learnt to estimate the position of the landmarks from the appearance of each part. We verify our algorithm with a set of experiments on human faces and these show the competitive performance of our method compared to existing methods.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\7FR946AR\\Kazemi and Sullivan - 2011 - Face Alignment with Part-Based Modeling.pdf}
}

@inproceedings{kazemiOneMillisecondFace2014,
  title = {One Millisecond Face Alignment with an Ensemble of Regression Trees},
  booktitle = {2014 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Kazemi, Vahid and Sullivan, Josephine},
  year = {2014},
  month = jun,
  pages = {1867--1874},
  publisher = {{IEEE}},
  address = {{Columbus, OH}},
  doi = {10.1109/CVPR.2014.241},
  abstract = {This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face's landmark positions directly from a sparse subset of pixel intensities, achieving super-realtime performance with high quality predictions. We present a general framework based on gradient boosting for learning an ensemble of regression trees that optimizes the sum of square error loss and naturally handles missing or partially labelled data. We show how using appropriate priors exploiting the structure of image data helps with efficient feature selection. Different regularization strategies and its importance to combat overfitting are also investigated. In addition, we analyse the effect of the quantity of training data on the accuracy of the predictions and explore the effect of data augmentation using synthesized data.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\7CIFBZ8V\\Kazemi and Sullivan - 2014 - One millisecond face alignment with an ensemble of.pdf},
  isbn = {978-1-4799-5118-5},
  language = {en}
}

@misc{kingDlib18Released,
  title = {Dlib 18.6 Released: {{Make}} Your Own Object Detector!},
  shorttitle = {Dlib 18.6 Released},
  author = {King, Davis},
  url = {http://blog.dlib.net/2014/02/dlib-186-released-make-your-own-object.html},
  urldate = {2020-03-09},
  abstract = {I just posted the next version of dlib, v18.6. ~There are a bunch of nice changes, but the most exciting addition is a tool for creating hi...},
  file = {C\:\\Users\\renan\\Zotero\\storage\\2H6RKN6V\\face_fhog_filters.png},
  note = {Library Catalog: blog.dlib.net}
}

@inproceedings{kluyverJupyterNotebooksPublishing2016,
  title = {Jupyter {{Notebooks}} \textendash{} a Publishing Format for Reproducible Computational Workflows},
  booktitle = {Positioning and Power in Academic Publishing: {{Players}}, Agents and Agendas},
  author = {Kluyver, Thomas and {Ragan-Kelley}, Benjamin and P{\'e}rez, Fernando and Granger, Brian and Bussonnier, Matthias and Frederic, Jonathan and Kelley, Kyle and Hamrick, Jessica and Grout, Jason and Corlay, Sylvain and Ivanov, Paul and Avila, Dami{\'a}n and Abdalla, Safia and Willing, Carol},
  editor = {Loizides, F. and Schmidt, B.},
  year = {2016},
  pages = {87--90},
  organization = {{IOS Press}}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  volume = {60},
  pages = {84--90},
  issn = {00010782},
  doi = {10.1145/3065386},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\3H9Q6WF2\\Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf},
  journal = {Communications of the ACM},
  language = {en},
  number = {6}
}

@inproceedings{lelanSpeakerDiarizationUnsupervised2016,
  title = {Speaker {{Diarization With Unsupervised Training Framework}}},
  booktitle = {41st {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}} 2016)},
  author = {Le Lan, Ga{\"e}l and Meignier, Sylvain and Charlet, Delphine and Del{\'e}glise, Paul},
  year = {2016},
  month = mar,
  pages = {5},
  address = {{Shanghai, China}},
  doi = {10.1109/ICASSP.2016.7472741},
  abstract = {This paper investigates single and cross-show diarization based on an unsupervised i-vector framework, on French TV and Radio corpora. This framework uses speaker clustering as a way to automatically select data from unlabeled corpora to train i-vector PLDA models. Performances between supervised and unsupervised models are compared. The experimental results on two distinct test corpora (one TV, one Radio) show that unsupervised models perform as good as supervised models for both tasks. Such results indicate that performing an effective cross-show diarization on new language or new domain data in the future should not depend on the availability of manually annotated data.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\WDAI2TCA\\Le Lan et al. - 2016 - Speaker Diarization With Unsupervised Training Fra.pdf}
}

@incollection{liangFaceAlignmentComponentBased2008,
  title = {Face {{Alignment Via Component}}-{{Based Discriminative Search}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2008},
  author = {Liang, Lin and Xiao, Rong and Wen, Fang and Sun, Jian},
  editor = {Forsyth, David and Torr, Philip and Zisserman, Andrew},
  year = {2008},
  volume = {5303},
  pages = {72--85},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-88688-4_6},
  abstract = {In this paper, we propose a component-based discriminative approach for face alignment without requiring initialization1. Unlike many approaches which locally optimize in a small range, our approach searches the face shape in a large range at the component level by a discriminative search algorithm. Specifically, a set of direction classifiers guide the search of the configurations of facial components among multiple detected modes of facial components. The direction classifiers are learned using a large number of aligned local patches and misaligned local patches from the training data. The discriminative search is extremely effective and able to find very good alignment results only in a few (2{$\sim$}3) search iterations. As the new approach gives excellent alignment results on the commonly used datasets (e.g., AR [18], FERET [21]) created under-controlled conditions, we evaluate our approach on a more challenging dataset containing over 1,700 well-labeled facial images with a large range of variations in pose, lighting, expression, and background. The experimental results show the superiority of our approach on both accuracy and efficiency.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\2L5R6TH7\\Liang et al. - 2008 - Face Alignment Via Component-Based Discriminative .pdf},
  isbn = {978-3-540-88685-3 978-3-540-88688-4},
  language = {en},
  note = {Series Title: Lecture Notes in Computer Science}
}

@patent{mcconnellMethodApparatusPattern1986,
  title = {Method of and Apparatus for Pattern Recognition},
  author = {McConnell, Robert K.},
  year = {1986},
  month = jan,
  url = {https://patents.google.com/patent/US4567610/en},
  urldate = {2020-03-13},
  assignee = {WAYLAND RES Inc},
  file = {C\:\\Users\\renan\\Zotero\\storage\\92WJGBYH\\McConnell - 1986 - Method of and apparatus for pattern recognition.pdf},
  keywords = {defined,histogram,number,reference,test},
  nationality = {US},
  note = {Library Catalog: Google Patents},
  number = {US4567610A}
}

@inproceedings{mccowanAMIMeetingCorpus2005,
  title = {The {{AMI Meeting Corpus}}},
  booktitle = {In: {{Proceedings Measuring Behavior}} 2005, 5th {{International Conference}} on {{Methods}} and {{Techniques}} in {{Behavioral Research}}. {{L}}.{{P}}.{{J}}.{{J}}. {{Noldus}}, {{F}}. {{Grieco}}, {{L}}.{{W}}.{{S}}. {{Loijens}} and {{P}}.{{H}}. {{Zimmerman}} ({{Eds}}.), {{Wageningen}}: {{Noldus Information Technology}}},
  author = {Mccowan, I. and Lathoud, G. and Lincoln, M. and Lisowska, A. and Post, W. and Reidsma, D. and Wellner, P.},
  year = {2005},
  abstract = {To support multi-disciplinary research in the AMI (Augmented Multi-party Interaction) project, a 100 hour corpus of meetings is being collected. This corpus is being recorded in several instrumented rooms equipped with a variety of microphones, video cameras, electronic pens, presentation slide capture and white-board capture devices. As well as real meetings, the corpus contains a significant proportion of scenario-driven meetings, which have been designed to elicit a rich range of realistic behaviors. To facilitate research, the raw data are being annotated at a number of levels including speech transcriptions, dialogue acts and summaries. The corpus is being distributed using a web server designed to allow convenient browsing and download of multimedia content and associated annotations. This article first overviews AMI research themes, then discusses corpus design, as well as data collection, annotation and distribution.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\W3JCAPJK\\Mccowan et al. - 2005 - The AMI Meeting Corpus.pdf;C\:\\Users\\renan\\Zotero\\storage\\FHP28W4E\\summary.html}
}

@article{mccullochLogicalCalculusIdeas1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S. and Pitts, Walter},
  year = {1943},
  month = dec,
  volume = {5},
  pages = {115--133},
  issn = {1522-9602},
  doi = {10.1007/BF02478259},
  abstract = {Because of the ``all-or-none'' character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\TQZHHSCE\\McCulloch and Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf},
  journal = {The bulletin of mathematical biophysics},
  language = {en},
  number = {4}
}

@inproceedings{mckinney-proc-scipy-2010,
  title = {Data {{Structures}} for {{Statistical Computing}} in {{Python}}},
  booktitle = {Proceedings of the 9th {{Python}} in {{Science Conference}}},
  author = {{Wes McKinney}},
  editor = {{van der Walt}, St{\'e}fan and {Jarrod Millman}},
  year = {2010},
  pages = {51--56}
}

@misc{mtheilerDeteccaoMarcadoresFaciais2019,
  title = {Detec{\c c}{\~a}o de {{Marcadores Faciais}} Pela Biblioteca {{Dlib}}},
  shorttitle = {English},
  author = {{MTheiler}},
  year = {2019},
  month = jan,
  url = {https://commons.wikimedia.org/wiki/File:Dlib-face_landmark_detection.jpg},
  urldate = {2020-02-25},
  file = {C\:\\Users\\renan\\Zotero\\storage\\NRIF95PR\\face_detections.jpg;C\:\\Users\\renan\\Zotero\\storage\\M34H965E\\FileDlib-face_landmark_detection.html}
}

@article{opencv_library,
  title = {The {{OpenCV}} Library},
  author = {Bradski, G.},
  year = {2000},
  citeulike-article-id = {2236121},
  journal = {Dr. Dobb's Journal of Software Tools},
  keywords = {bibtex-import},
  posted-at = {2008-01-15 19:21:54},
  priority = {4}
}

@article{oxenhamPitchPerception2012,
  title = {Pitch {{Perception}}},
  author = {Oxenham, Andrew J.},
  year = {2012},
  month = sep,
  volume = {32},
  pages = {13335--13338},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3815-12.2012},
  abstract = {Pitch is one of the primary auditory sensations and plays a defining role in music, speech, and auditory scene analysis. Although the main physical correlate of pitch is acoustic periodicity, or repetition rate, there are many interactions that complicate the relationship between the physical stimulus and the perception of pitch. In particular, the effects of other acoustic parameters on pitch judgments, and the complex interactions between perceptual organization and pitch, have uncovered interesting perceptual phenomena that should help to reveal the underlying neural mechanisms.},
  copyright = {Copyright \textcopyright{} 2012 the authors 0270-6474/12/3213335-04\$15.00/0},
  file = {C\:\\Users\\renan\\Zotero\\storage\\4UP3WFTR\\Oxenham - 2012 - Pitch Perception.pdf;C\:\\Users\\renan\\Zotero\\storage\\GIDP5F5Y\\13335.html},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {39},
  pmid = {23015422}
}

@article{peddagollaLaneDetectionAutonomous,
  title = {On the {{Lane Detection}} for {{Autonomous Driving}}: {{A Computational Experimental Study}} on {{Performance}} of {{Edge Detectors}}},
  author = {Peddagolla, Balakrishna Yadav and Kathiresan, Santhosh Sivan and Doshi, Ninad C and Prabhu, Nikhil S and Zadeh, Mehrdad H},
  pages = {10},
  abstract = {In this paper, a computational analysis of various edge detectors like Sobel, Prewitt, LoG, Canny and Roberts is performed. Many researchers have used these methods for edge detections. However, there is a need to conduct a thorough study on the edge detection techniques with respect to execution time that is a very important factor for realtime decisions in automated driving. Edge detection is a tectonic step for lane detection in computer vision of Automated Driving, so it is important to have better understanding of various edge detectors. We use a comprehensive set of performance metrics for evaluating these edge detectors in automated driving setting. The metrics include Entropy, Mean Squared Error (MSE), Peak Signal-to-noise Ratio (PSNR), and Structural Similarity Index (SSI). Along with these parameters, computational time for detection of lane using each of these edge operators is also used as an important factor for automated driving. Several trends were observed within the results of this study that enables developer and researcher to improve lane detection for automated driving. For instance, results show that the performance of Sobel operator overmatched the combination of performance and execution time of all other edge operators including Canny.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\EX3WZWS6\\Peddagolla et al. - On the Lane Detection for Autonomous Driving A Co.pdf},
  language = {en}
}

@article{phamPredictingHealthcareTrajectories2017,
  title = {Predicting Healthcare Trajectories from Medical Records: {{A}} Deep Learning Approach},
  shorttitle = {Predicting Healthcare Trajectories from Medical Records},
  author = {Pham, Trang and Tran, Truyen and Phung, Dinh and Venkatesh, Svetha},
  year = {2017},
  month = may,
  volume = {69},
  pages = {218--229},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2017.04.001},
  abstract = {Personalized predictive medicine necessitates the modeling of patient illness and care processes, which inherently have long-term temporal dependencies. Healthcare observations, stored in electronic medical records are episodic and irregular in time. We introduce DeepCare, an end-to-end deep dynamic neural network that reads medical records, stores previous illness history, infers current illness states and predicts future medical outcomes. At the data level, DeepCare represents care episodes as vectors and models patient health state trajectories by the memory of historical records. Built on Long Short-Term Memory (LSTM), DeepCare introduces methods to handle irregularly timed events by moderating the forgetting and consolidation of memory. DeepCare also explicitly models medical interventions that change the course of illness and shape future medical risk. Moving up to the health state level, historical and present health states are then aggregated through multiscale temporal pooling, before passing through a neural network that estimates future outcomes. We demonstrate the efficacy of DeepCare for disease progression modeling, intervention recommendation, and future risk prediction. On two important cohorts with heavy social and economic burden \textendash{} diabetes and mental health \textendash{} the results show improved prediction accuracy.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\RKBNZHZS\\Pham et al. - 2017 - Predicting healthcare trajectories from medical re.pdf;C\:\\Users\\renan\\Zotero\\storage\\QT38T3K5\\S1532046417300710.html},
  journal = {Journal of Biomedical Informatics},
  keywords = {Electronic medical records,Healthcare processes,Irregular timing,Long-Short Term Memory,Predictive medicine},
  language = {en}
}

@article{rosenblattPerceptronProbabilisticModel1958,
  title = {The {{Perceptron}}: {{A Probabilistic Model}} for {{Information Storage}} and {{Organization}} in {{The Brain}}},
  shorttitle = {The {{Perceptron}}},
  author = {Rosenblatt, F.},
  year = {1958},
  pages = {65--386},
  abstract = {If we are eventually to understand the capability of higher organisms for perceptual recognition, generalization, recall, and thinking, we must first have answers to three fundamental questions: 1. How is information about the physical world sensed, or detected, by the biological system? 2. In what form is information stored, or remembered? 3. How does information contained in storage, or in memory, influence recognition and behavior? The first of these questions is in the},
  file = {C\:\\Users\\renan\\Zotero\\storage\\85I9GKY8\\Rosenblatt - 1958 - The Perceptron A Probabilistic Model for Informat.pdf;C\:\\Users\\renan\\Zotero\\storage\\RHY68RX7\\summary.html},
  journal = {Psychological Review}
}

@inproceedings{sellSpeakerDiarizationPlda2014,
  title = {Speaker Diarization with Plda I-Vector Scoring and Unsupervised Calibration},
  booktitle = {2014 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Sell, Gregory and {Garcia-Romero}, Daniel},
  year = {2014},
  month = dec,
  pages = {413--417},
  issn = {null},
  doi = {10.1109/SLT.2014.7078610},
  abstract = {Speaker diarization via unsupervised i-vector clustering has gained popularity in recent years. In this approach, i-vectors are extracted from short clips of speech segmented from a larger multi-speaker conversation and organized into speaker clusters, typically according to their cosine score. In this paper, we propose a system that incorporates probabilistic linear discriminant analysis (PLDA) for i-vector scoring, a method already frequently utilized in speaker recognition tasks, and uses unsupervised calibration of the PLDA scores to determine the clustering stopping criterion. We also demonstrate that denser sampling in the i-vector space with overlapping temporal segments provides a gain in the diarization task. We test our system on the CALLHOME conversational telephone speech corpus, which includes multiple languages and a varying number of speakers, and we show that PLDA scoring outperforms the same system with cosine scoring, and that overlapping segments reduce diarization error rate (DER) as well.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\PMETV7U2\\Sell and Garcia-Romero - 2014 - Speaker diarization with plda i-vector scoring and.pdf;C\:\\Users\\renan\\Zotero\\storage\\6Q9TFBAT\\7078610.html},
  keywords = {calibration,Calibration,CALLHOME conversational telephone speech corpus,cosine score,denser sampling,Density estimation robust algorithm,DER,diarization error rate,multispeaker conversation,pattern clustering,PLDA i-vector scoring,Principal component analysis,probabilistic linear discriminant analysis,sampling methods,segmented speech,speaker clusters,speaker diarization,speaker recognition,Speaker recognition,speaker recognition tasks,Speech,Speech processing,unsupervised calibration,unsupervised i-vector clustering}
}

@inproceedings{snyderXVectorsRobustDNN2018,
  title = {X-{{Vectors}}: {{Robust DNN Embeddings}} for {{Speaker Recognition}}},
  shorttitle = {X-{{Vectors}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Snyder, David and {Garcia-Romero}, Daniel and Sell, Gregory and Povey, Daniel and Khudanpur, Sanjeev},
  year = {2018},
  month = apr,
  pages = {5329--5333},
  publisher = {{IEEE}},
  address = {{Calgary, AB}},
  doi = {10.1109/ICASSP.2018.8461375},
  abstract = {In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\95UVGLQV\\Snyder et al. - 2018 - X-Vectors Robust DNN Embeddings for Speaker Recog.pdf},
  isbn = {978-1-5386-4658-8},
  language = {en}
}

@article{tensorflow2015-whitepaper,
  title = {{{TensorFlow}}: {{Large}}-Scale {{Machine Learning}} on {{Heterogeneous Systems}}},
  author = {Abadi, Mart{\'i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'e}, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'e}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2015},
  url = {https://www.tensorflow.org/},
  abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, generalpurpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous ``parameter server'' designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.}
}

@inproceedings{wangSpeakerDiarizationLSTM2018,
  title = {Speaker {{Diarization}} with {{LSTM}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Wang, Quan and Downey, Carlton and Wan, Li and Mansfield, Philip Andrew and Moreno, Ignacio Lopz},
  year = {2018},
  month = apr,
  pages = {5239--5243},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2018.8462628},
  abstract = {For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and speaker diarization applications. However, mirroring the rise of deep learning in various domains, neural network based audio embeddings, also known as d-vectors, have consistently demonstrated superior speaker verification performance. In this paper, we build on the success of d-vector based speaker verification systems to develop a new d-vector based approach to speaker diarization. Specifically, we combine LSTM-based d-vector audio embeddings with recent work in non-parametric clustering to obtain a state-of-the-art speaker diarization system. Our system is evaluated on three standard public datasets, suggesting that d-vector based diarization systems offer significant advantages over traditional i-vector based systems. We achieved a 12.0\% diarization error rate on NIST SRE 2000 CALLHOME, while our model is trained with out-of-domain data from voice search logs.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\YSEJMTVN\\Wang et al. - 2018 - Speaker Diarization with LSTM.pdf;C\:\\Users\\renan\\Zotero\\storage\\7QZAM5ZR\\8462628.html},
  keywords = {audio embedding,audio signal processing,Clustering algorithms,d-vector based diarization systems,d-vector based speaker verification systems,deep learning,diarization error rate,Feature extraction,i-vector based audio embedding techniques,i-vector based systems,LSTM,LSTM-based d-vector audio embeddings,Machine learning,Neural networks,NIST,recurrent neural nets,Speaker diarization,speaker diarization system,speaker recognition,spectral clustering,Voice activity detection}
}

@book{werbosRegressionNewTools1975,
  title = {Beyond {{Regression}}: {{New Tools}} for {{Prediction}} and {{Analysis}} in the {{Behavioral Sciences}}},
  shorttitle = {Beyond {{Regression}}},
  author = {Werbos, Paul John},
  year = {1975},
  publisher = {{Harvard University}},
  googlebooks = {z81XmgEACAAJ},
  language = {en}
}

@article{widrow30YearsAdaptive1990,
  title = {30 Years of Adaptive Neural Networks: Perceptron, {{Madaline}}, and Backpropagation},
  shorttitle = {30 Years of Adaptive Neural Networks},
  author = {Widrow, B. and Lehr, M.A.},
  year = {Sept./1990},
  volume = {78},
  pages = {1415--1442},
  issn = {00189219},
  doi = {10.1109/5.58323},
  file = {C\:\\Users\\renan\\Zotero\\storage\\X79K35A7\\Widrow and Lehr - 1990 - 30 years of adaptive neural networks perceptron, .pdf},
  journal = {Proceedings of the IEEE},
  language = {en},
  number = {9}
}

@inproceedings{xiangxinzhuFaceDetectionPose2012,
  title = {Face Detection, Pose Estimation, and Landmark Localization in the Wild},
  booktitle = {2012 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {{Xiangxin Zhu} and Ramanan, D.},
  year = {2012},
  month = jun,
  pages = {2879--2886},
  publisher = {{IEEE}},
  address = {{Providence, RI}},
  doi = {10.1109/CVPR.2012.6248014},
  abstract = {We present a unified model for face detection, pose estimation, and landmark estimation in real-world, cluttered images. Our model is based on a mixtures of trees with a shared pool of parts; we model every facial landmark as a part and use global mixtures to capture topological changes due to viewpoint. We show that tree-structured models are surprisingly effective at capturing global elastic deformation, while being easy to optimize unlike dense graph structures. We present extensive results on standard face benchmarks, as well as a new ``in the wild'' annotated dataset, that suggests our system advances the state-of-theart, sometimes considerably, for all three tasks. Though our model is modestly trained with hundreds of faces, it compares favorably to commercial systems trained with billions of examples (such as Google Picasa and face.com).},
  file = {C\:\\Users\\renan\\Zotero\\storage\\YQPHERVB\\Xiangxin Zhu and Ramanan - 2012 - Face detection, pose estimation, and landmark loca.pdf},
  isbn = {978-1-4673-1228-8 978-1-4673-1226-4 978-1-4673-1227-1},
  language = {en}
}

@article{yudinObjectDetectionDeep2019,
  title = {Object {{Detection}} with {{Deep Neural Networks}} for {{Reinforcement Learning}} in the {{Task}} of {{Autonomous Vehicles Path Planning}} at the {{Intersection}}},
  author = {Yudin, D. A. and Skrynnik, A. and Krishtopik, A. and Belkin, I. and Panov, A. I.},
  year = {2019},
  month = oct,
  volume = {28},
  pages = {283--295},
  issn = {1934-7898},
  doi = {10.3103/S1060992X19040118},
  abstract = {Among a number of problems in the behavior planning of an unmanned vehicle the central one is movement in difficult areas. In particular, such areas are intersections at which direct interaction with other road agents takes place. In our work, we offer a new approach to train of the intelligent agent that simulates the behavior of an unmanned vehicle, based on the integration of reinforcement learning and computer vision. Using full visual information about the road intersection obtained from aerial photographs, it is studied automatic detection the relative positions of all road agents with various architectures of deep neural networks (YOLOv3, Faster R-CNN, RetinaNet, Cascade R-CNN, Mask R-CNN, Cascade Mask R-CNN). The possibilities of estimation of the vehicle orientation angle based on a convolutional neural network are also investigated. Obtained additional features are used in the modern effective reinforcement learning methods of Soft Actor Critic and Rainbow, which allows to accelerate the convergence of its learning process. To demonstrate the operation of the developed system, an intersection simulator was developed, at which a number of model experiments were carried out.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\TUTPLGUA\\Yudin et al. - 2019 - Object Detection with Deep Neural Networks for Rei.pdf},
  journal = {Optical Memory and Neural Networks},
  language = {en},
  number = {4}
}

@article{yuilleFeatureExtractionFaces1992,
  title = {Feature Extraction from Faces Using Deformable Templates},
  author = {Yuille, Alan L. and Hallinan, Peter W. and Cohen, David S.},
  year = {1992},
  month = aug,
  volume = {8},
  pages = {99--111},
  publisher = {{Kluwer Academic Publishers}},
  issn = {1573-1405},
  doi = {10.1007/BF00127169},
  abstract = {We propose a method for detecting and describing features of faces using deformable templates. The feature of interest, an eye for example, is described by a parameterized template. An energy function is defined which links edges, peaks, and valleys in the image intensity to corresponding properties of the template. The template then interacts dynamically with the image by altering its parameter values to minimize the energy function, thereby deforming itself to find the best fit. The final parametr values can be used as descriptors for the feature. We illustrate this method by showing deformable templates detecting eyes and mouths in real images. We demonstrate their ability for tracking features.},
  copyright = {1992 Kluwer Academic Publishers},
  file = {C\:\\Users\\renan\\Zotero\\storage\\ZC2Q32BN\\Yuille et al. - 1992 - Feature extraction from faces using deformable tem.pdf;C\:\\Users\\renan\\Zotero\\storage\\S7GJM99Y\\10.html},
  journal = {International Journal of Computer Vision},
  language = {en},
  note = {Company: Springer
Distributor: Springer
Institution: Springer
Label: Springer},
  number = {2}
}

@article{zewoudieUseLongtermFeatures2018,
  title = {The Use of Long-Term Features for {{GMM}}- and i-Vector-Based Speaker Diarization Systems},
  author = {Zewoudie, Abraham Woubie and Luque, Jordi and Hernando, Javier},
  year = {2018},
  month = sep,
  volume = {2018},
  pages = {14},
  issn = {1687-4722},
  doi = {10.1186/s13636-018-0140-x},
  abstract = {Several factors contribute to the performance of speaker diarization systems. For instance, the appropriate selection of speech features is one of the key aspects that affect speaker diarization systems. The other factors include the techniques employed to perform both segmentation and clustering. While the static mel frequency cepstral coefficients are the most widely used features in speech-related tasks including speaker diarization, several studies have shown the benefits of augmenting regular speech features with the static ones.},
  file = {C\:\\Users\\renan\\Zotero\\storage\\GXP27EB6\\Zewoudie et al. - 2018 - The use of long-term features for GMM- and i-vecto.pdf},
  journal = {EURASIP Journal on Audio, Speech, and Music Processing},
  language = {en},
  number = {1}
}


